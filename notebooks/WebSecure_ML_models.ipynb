{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        " # WebSecure: Syst√®me Avanc√© d'Analyse de Vuln√©rabilit√©s Web\n",
        "    \n",
        "  **Auteurs: ELGARCH Youssef et IBNOU-KADY Nisrine**\n",
        "    \n",
        "  ## Introduction\n",
        "\n",
        "  Ce notebook pr√©sente le syst√®me WebSecure, une plateforme d'analyse de vuln√©rabilit√©s web qui combine des techniques d'apprentissage automatique et d'analyse de sites web pour d√©tecter les risques de s√©curit√© potentiels.\n",
        "  \n",
        "  Le syst√®me est capable de :\n",
        "    - Analyser les sites web pour d√©tecter les vuln√©rabilit√©s\n",
        "    - Classifier les niveaux de risque (Faible, Mod√©r√©, Important, Critique),\n",
        "    - D√©tecter les anomalies de s√©curit√©,\n",
        "    - G√©n√©rer des rapports d√©taill√©s avec recommandations,\n",
        "    - S'int√©grer avec une application web pour une utilisation simplifi√©e,\n",
        "  \n",
        "  ### Objectifs du projet\n",
        "  \n",
        "    1. D√©velopper un mod√®le de machine learning pour pr√©dire les risques de s√©curit√©\n",
        "    2. Cr√©er un syst√®me d'extraction automatique des caract√©ristiques de s√©curit√© d'un site web\n",
        "    3. Concevoir une interface utilisateur intuitive pour l'analyse de s√©curit√©\n",
        "    4. Produire des rapports pertinents et actionnables pour les utilisateurs\n",
        "    \n",
        " ### Pr√©requis et d√©pendances\n",
        "    \n",
        "    Le notebook utilise plusieurs biblioth√®ques Python pour l'analyse et l'apprentissage automatique :\n"
      ],
      "metadata": {
        "id": "-O1EOafgix4W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ggbmufLwiEUx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bpl0XnuM__fl",
        "outputId": "01624ad4-480b-4f9a-e465-6101b0ff5404"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import joblib\n",
        "import warnings\n",
        "import time\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.feature_selection import SelectKBest, chi2, f_classif, mutual_info_classif\n",
        "from sklearn.decomposition import PCA, TruncatedSVD\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, roc_auc_score, precision_recall_curve\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, VotingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "from sklearn.pipeline import Pipeline\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Configuration des biblioth√®ques NLTK\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "Mc2kKL3788gc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6a6420e-e211-4051-c42b-3cb28b1efd32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SECTION 1: PR√âPARATION ET PR√âTRAITEMENT DES DONN√âES"
      ],
      "metadata": {
        "id": "cH8AfHqS8xMr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DataPreprocessor:\n",
        "    \"\"\"Classe pour le pr√©traitement des donn√©es de vuln√©rabilit√©\"\"\"\n",
        "\n",
        "    def __init__(self, config=None):\n",
        "        self.config = config or {\n",
        "            'text_cleaning': True,\n",
        "            'handle_missing': 'fill_empty',  # 'drop', 'fill_empty', 'fill_median'\n",
        "            'scaling_method': 'standard',    # 'standard', 'minmax', 'robust'\n",
        "            'feature_selection': True,\n",
        "            'n_features': 20,\n",
        "            'stemming': True,\n",
        "            'language': 'french',\n",
        "            'apply_smote': False\n",
        "        }\n",
        "        # Explicitly set stemming as an attribute\n",
        "        self.stemming = self.config['stemming']\n",
        "        self.scaler = None\n",
        "        self.feature_selector = None\n",
        "\n",
        "        # T√©l√©chargement des donn√©es NLTK si n√©cessaire\n",
        "        try:\n",
        "            nltk.data.find('tokenizers/punkt')\n",
        "        except LookupError:\n",
        "            nltk.download('punkt')\n",
        "\n",
        "        try:\n",
        "            nltk.data.find('corpora/stopwords')\n",
        "        except LookupError:\n",
        "            nltk.download('stopwords')\n",
        "\n",
        "        self.stemmer = SnowballStemmer(self.config['language']) if self.stemming else None\n",
        "        self.stop_words = list(stopwords.words(self.config['language'])) if self.stemming else None\n",
        "        self.stemmer = SnowballStemmer(self.config['language']) if self.stemming else None\n",
        "        self.stop_words = list(stopwords.words(self.config['language'])) if self.stemming else None\n",
        "\n",
        "\n",
        "    def load_and_preprocess(self, file_path):\n",
        "        \"\"\"Charge et pr√©traite les donn√©es depuis un fichier CSV\"\"\"\n",
        "        print(f\"\\nüìÇ Chargement des donn√©es depuis: {file_path}...\")\n",
        "        try:\n",
        "            df = pd.read_csv(file_path)\n",
        "            print(f\"   ‚úì Donn√©es charg√©es: {df.shape[0]} entr√©es, {df.shape[1]} colonnes\")\n",
        "        except FileNotFoundError:\n",
        "            raise FileNotFoundError(f\"‚ö†Ô∏è Fichier non trouv√©: {file_path}\")\n",
        "        except Exception as e:\n",
        "            raise Exception(f\"‚ö†Ô∏è Erreur lors du chargement du fichier: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "        # Pr√©traitement\n",
        "        print(\"   - Pr√©traitement des donn√©es...\")\n",
        "        df = self.handle_missing_values(df)\n",
        "\n",
        "        # Normalisation des niveaux de risque et d'impact (added code)\n",
        "        risk_level_map = {\n",
        "            'Critique': 3,\n",
        "            'Important': 2,\n",
        "            'Mod√©r√©': 1,\n",
        "            'No Risk Level': 0  # Include mapping for 'No Risk Level' if present\n",
        "        }\n",
        "\n",
        "        impact_level_map = {\n",
        "            'Critique': 3,\n",
        "            'Important': 2,\n",
        "            'Mod√©r√©': 1,\n",
        "            'No Impact Level': 0  # Include mapping for 'No Impact Level' if present\n",
        "        }\n",
        "\n",
        "        df['Risk Level Numeric'] = df['Risk Level'].map(risk_level_map)\n",
        "        df['Impact Level Numeric'] = df['Impact Level'].map(impact_level_map)\n",
        "\n",
        "        # Combiner les colonnes de texte pour l'extraction des caract√©ristiques\n",
        "        df['Combined_Text'] = df['Vulnerability Summary'] + ' ' + df['Affected Systems'] + ' ' + df['Solution']\n",
        "\n",
        "        # Convertir en minuscules et supprimer les caract√®res sp√©ciaux (facultatif)\n",
        "        if self.config['text_cleaning']:\n",
        "            df['Combined_Text'] = df['Combined_Text'].apply(self.clean_text)\n",
        "\n",
        "        print(\"   ‚úì Donn√©es pr√©trait√©es.\")\n",
        "        return df\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def handle_missing_values(self, df):\n",
        "        \"\"\"G√®re les valeurs manquantes dans le DataFrame\"\"\"\n",
        "        if self.config['handle_missing'] == 'drop':\n",
        "           df = df.dropna()\n",
        "        elif self.config['handle_missing'] == 'fill_empty':\n",
        "        # Fill missing values with empty strings for string columns only\n",
        "             for col in df.select_dtypes(include=['object']).columns:\n",
        "                 df[col] = df[col].fillna('')\n",
        "        # Fill missing values with 0 for numeric columns only\n",
        "             for col in df.select_dtypes(include=np.number).columns:\n",
        "                 df[col] = df[col].fillna(0)\n",
        "        elif self.config['handle_missing'] == 'fill_median':\n",
        "             for col in df.select_dtypes(include=np.number).columns:\n",
        "                 df[col] = df[col].fillna(df[col].median())\n",
        "        return df\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def clean_text(self, text):\n",
        "        \"\"\"Nettoie le texte en supprimant les caract√®res sp√©ciaux et en convertissant en minuscules\"\"\"\n",
        "        text = text.lower()\n",
        "        text = re.sub(r'[^\\w\\s]', '', text)  # Supprime les caract√®res sp√©ciaux\n",
        "        text = text.strip()\n",
        "        if self.stemming:  # Applique le stemming si activ√©\n",
        "            tokens = nltk.word_tokenize(text)\n",
        "            stemmed_tokens = [self.stemmer.stem(token) for token in tokens if token not in self.stop_words]\n",
        "            text = ' '.join(stemmed_tokens)\n",
        "        return text\n",
        "\n",
        "    def extract_features(self, df):\n",
        "        \"\"\"Extrait des caract√©ristiques pour l'analyse de vuln√©rabilit√©\"\"\"\n",
        "        print(\"\\nüîß Extraction des caract√©ristiques...\")\n",
        "\n",
        "         # V√©rification des NaN dans Risk Level Numeric avant tout traitement\n",
        "        if df['Risk Level Numeric'].isna().any():\n",
        "            print(\"   ‚ö†Ô∏è Valeurs manquantes d√©tect√©es dans Risk Level Numeric. Remplacement par 0...\")\n",
        "            df['Risk Level Numeric'] = df['Risk Level Numeric'].fillna(0)\n",
        "\n",
        "    # Caract√©ristiques textuelles\n",
        "        if 'Combined_Text' in df.columns:\n",
        "           text_column = 'Combined_Text'\n",
        "        else:\n",
        "           text_column = 'Vulnerability Summary'\n",
        "\n",
        "        print(f\"   - Utilisation de la colonne '{text_column}' pour l'extraction textuelle\")\n",
        "\n",
        "    # Vectorisation des descriptions\n",
        "        tfidf = TfidfVectorizer(max_features=1000, stop_words=self.stop_words)\n",
        "        X_text = tfidf.fit_transform(df[text_column])\n",
        "        print(f\"   ‚úì Caract√©ristiques textuelles extraites: {X_text.shape[1]} dimensions\")\n",
        "\n",
        "    # Enregistrement du vectoriseur\n",
        "        joblib.dump(tfidf, 'models/tfidf_vectorizer.pkl')\n",
        "\n",
        "    # Extraction de caract√©ristiques bas√©es sur les r√®gles\n",
        "        features = []\n",
        "        print(\"   - Extraction des caract√©ristiques bas√©es sur les r√®gles...\")\n",
        "\n",
        "        for idx, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"   üîÑ Traitement des entr√©es\"):\n",
        "           affected_systems = str(row['Affected Systems']).lower()\n",
        "           vuln_summary = str(row['Vulnerability Summary']).lower()\n",
        "\n",
        "           feature_dict = {\n",
        "                # Syst√®mes affect√©s\n",
        "                'has_windows': 1 if 'windows' in affected_systems else 0,\n",
        "                'has_linux': 1 if 'linux' in affected_systems else 0,\n",
        "                'has_macos': 1 if ('macos' in affected_systems or 'mac os' in affected_systems) else 0,\n",
        "                'has_android': 1 if 'android' in affected_systems else 0,\n",
        "                'has_ios': 1 if ('ios' in affected_systems or 'iphone' in affected_systems or 'ipad' in affected_systems) else 0,\n",
        "                'has_wordpress': 1 if 'wordpress' in affected_systems else 0,\n",
        "                'has_joomla': 1 if 'joomla' in affected_systems else 0,\n",
        "                'has_drupal': 1 if 'drupal' in affected_systems else 0,\n",
        "                'has_oracle': 1 if 'oracle' in affected_systems else 0,\n",
        "                'has_microsoft': 1 if 'microsoft' in affected_systems else 0,\n",
        "                'has_adobe': 1 if 'adobe' in affected_systems else 0,\n",
        "                'has_plugin': 1 if 'plugin' in affected_systems or 'extension' in affected_systems else 0,\n",
        "                'has_server': 1 if 'server' in affected_systems or 'serveur' in affected_systems else 0,\n",
        "                'has_web': 1 if 'web' in affected_systems else 0,\n",
        "                'has_mobile': 1 if 'mobile' in affected_systems else 0,\n",
        "                'has_iot': 1 if 'iot' in affected_systems or 'internet of things' in affected_systems else 0,\n",
        "                'has_cloud': 1 if 'cloud' in affected_systems or 'aws' in affected_systems or 'azure' in affected_systems else 0,\n",
        "\n",
        "                # Types de vuln√©rabilit√©s\n",
        "                'has_injection': 1 if 'injection' in vuln_summary else 0,\n",
        "                'has_xss': 1 if 'xss' in vuln_summary or 'cross-site' in vuln_summary or 'script' in vuln_summary else 0,\n",
        "                'has_sql': 1 if 'sql' in vuln_summary else 0,\n",
        "                'has_authentication': 1 if 'authentification' in vuln_summary or 'authentication' in vuln_summary else 0,\n",
        "                'has_authorization': 1 if 'autorisation' in vuln_summary or 'authorization' in vuln_summary else 0,\n",
        "                'has_privilege': 1 if 'privil√®ge' in vuln_summary or 'privilege' in vuln_summary else 0,\n",
        "                'has_code_execution': 1 if 'ex√©cution de code' in vuln_summary or 'code execution' in vuln_summary else 0,\n",
        "                'has_arbitrary_code': 1 if 'arbitraire' in vuln_summary or 'arbitrary' in vuln_summary else 0,\n",
        "                'has_buffer': 1 if 'buffer' in vuln_summary or 'tampon' in vuln_summary else 0,\n",
        "                'has_overflow': 1 if 'overflow' in vuln_summary or 'd√©bordement' in vuln_summary else 0,\n",
        "                'has_dos': 1 if 'd√©ni de service' in vuln_summary or 'denial of service' in vuln_summary or 'dos' in vuln_summary else 0,\n",
        "                'has_csrf': 1 if 'csrf' in vuln_summary or 'cross-site request' in vuln_summary else 0,\n",
        "                'has_path_traversal': 1 if 'path traversal' in vuln_summary or 'directory traversal' in vuln_summary else 0,\n",
        "                'has_file_inclusion': 1 if 'file inclusion' in vuln_summary or 'inclusion de fichier' in vuln_summary else 0,\n",
        "                'has_encryption': 1 if 'encryption' in vuln_summary or 'chiffrement' in vuln_summary else 0,\n",
        "                'has_ssl': 1 if 'ssl' in vuln_summary or 'tls' in vuln_summary else 0,\n",
        "                'has_mitm': 1 if 'man in the middle' in vuln_summary or 'mitm' in vuln_summary or 'homme du milieu' in vuln_summary else 0,\n",
        "                'has_bypass': 1 if 'bypass' in vuln_summary or 'contournement' in vuln_summary else 0,\n",
        "                'has_backdoor': 1 if 'backdoor' in vuln_summary or 'porte d√©rob√©e' in vuln_summary else 0,\n",
        "                'has_zero_day': 1 if 'zero day' in vuln_summary or 'zero-day' in vuln_summary or '0-day' in vuln_summary else 0,\n",
        "\n",
        "                # M√©triques existantes\n",
        "                'risk_level': row['Risk Level Numeric'],\n",
        "                'impact_level': row['Impact Level Numeric']\n",
        "            }\n",
        "\n",
        "           features.append(feature_dict)\n",
        "\n",
        "        # Conversion en DataFrame\n",
        "        X_features = pd.DataFrame(features)\n",
        "        print(f\"   ‚úì Caract√©ristiques bas√©es sur les r√®gles extraites: {X_features.shape[1]} dimensions\")\n",
        "\n",
        "         # V√©rifier et traiter les valeurs NaN AVANT le scaling\n",
        "        if X_features.isna().any().any():\n",
        "           print(\"   ‚ö†Ô∏è Valeurs manquantes d√©tect√©es dans les caract√©ristiques. Remplacement par 0...\")\n",
        "           X_features = X_features.fillna(0)  # Remplacer toutes les valeurs NaN par 0\n",
        "\n",
        "        # Application du scaling\n",
        "        if self.config['scaling_method'] == 'standard':\n",
        "            self.scaler = StandardScaler()\n",
        "        elif self.config['scaling_method'] == 'minmax':\n",
        "            self.scaler = MinMaxScaler()\n",
        "        elif self.config['scaling_method'] == 'robust':\n",
        "            self.scaler = RobustScaler()\n",
        "        else:\n",
        "            self.scaler = StandardScaler()\n",
        "\n",
        "        X_features_scaled = self.scaler.fit_transform(X_features)\n",
        "        print(f\"   ‚úì Scaling appliqu√©: {self.config['scaling_method']}\")\n",
        "\n",
        "        # Enregistrement du scaler\n",
        "        joblib.dump(self.scaler, 'models/feature_scaler.pkl')\n",
        "\n",
        "\n",
        "\n",
        "         # Lorsque vous arrivez √† SelectKBest, utilisez la variable df mise √† jour:\n",
        "        if self.config['feature_selection'] and X_features.shape[1] > self.config['n_features']:\n",
        "            print(f\"   - S√©lection des {self.config['n_features']} meilleures caract√©ristiques...\")\n",
        "\n",
        "            self.feature_selector = SelectKBest(f_classif, k=self.config['n_features'])\n",
        "        # Utiliser df['Risk Level Numeric'] qui a d√©j√† √©t√© nettoy√© des NaN\n",
        "            X_features_selected = self.feature_selector.fit_transform(X_features_scaled, df['Risk Level Numeric'])\n",
        "\n",
        "            # Enregistrement du s√©lecteur\n",
        "            joblib.dump(self.feature_selector, 'models/feature_selector.pkl')\n",
        "\n",
        "            print(f\"   ‚úì S√©lection de caract√©ristiques appliqu√©e: {X_features_selected.shape[1]} dimensions retenues\")\n",
        "\n",
        "            # R√©cup√©rer et afficher les noms des caract√©ristiques s√©lectionn√©es\n",
        "            selected_indices = self.feature_selector.get_support(indices=True)\n",
        "            selected_features = X_features.columns[selected_indices].tolist()\n",
        "            print(f\"   üìã Caract√©ristiques s√©lectionn√©es: {selected_features}\")\n",
        "\n",
        "            return X_features_selected, X_text, selected_features\n",
        "        else:\n",
        "            return X_features_scaled, X_text, X_features.columns.tolist()\n",
        "\n",
        "    def apply_sampling(self, X, y):\n",
        "        \"\"\"Applique des techniques de sur-√©chantillonnage ou sous-√©chantillonnage\"\"\"\n",
        "        if not self.config['apply_smote']:\n",
        "            return X, y\n",
        "\n",
        "        print(\"\\n‚öñÔ∏è Application de techniques d'√©quilibrage des classes...\")\n",
        "\n",
        "        # Affichage de la distribution initiale\n",
        "        class_counts = pd.Series(y).value_counts().sort_index()\n",
        "        print(f\"   - Distribution initiale des classes: {class_counts.to_dict()}\")\n",
        "\n",
        "        # Application de SMOTE\n",
        "        smote = SMOTE(random_state=42)\n",
        "        X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "\n",
        "        # Affichage de la distribution apr√®s SMOTE\n",
        "        class_counts_after = pd.Series(y_resampled).value_counts().sort_index()\n",
        "        print(f\"   ‚úì Distribution apr√®s SMOTE: {class_counts_after.to_dict()}\")\n",
        "\n",
        "        return X_resampled, y_resampled"
      ],
      "metadata": {
        "id": "gQ0a6bTqCEc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EXTRACTION DES CARACT√âRISTIQUES"
      ],
      "metadata": {
        "id": "fRfXfGt4-cBw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(self, df):\n",
        "    \"\"\"Extrait des caract√©ristiques pour l'analyse de vuln√©rabilit√©\"\"\"\n",
        "    print(\"\\nüîß Extraction des caract√©ristiques...\")\n",
        "\n",
        "    # Caract√©ristiques textuelles\n",
        "    if 'Combined_Text' in df.columns:\n",
        "        text_column = 'Combined_Text'\n",
        "    else:\n",
        "        text_column = 'Vulnerability Summary'\n",
        "\n",
        "    print(f\"   - Utilisation de la colonne '{text_column}' pour l'extraction textuelle\")\n",
        "\n",
        "    # Vectorisation des descriptions\n",
        "    tfidf = TfidfVectorizer(max_features=1000, stop_words=self.stop_words)\n",
        "    X_text = tfidf.fit_transform(df[text_column])\n",
        "    print(f\"   ‚úì Caract√©ristiques textuelles extraites: {X_text.shape[1]} dimensions\")\n",
        "\n",
        "    # Enregistrement du vectoriseur\n",
        "    joblib.dump(tfidf, 'models/tfidf_vectorizer.pkl')\n",
        "\n",
        "    # Extraction de caract√©ristiques bas√©es sur les r√®gles\n",
        "    features = []\n",
        "    print(\"   - Extraction des caract√©ristiques bas√©es sur les r√®gles...\")\n",
        "\n",
        "    for idx, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"   üîÑ Traitement des entr√©es\"):\n",
        "        affected_systems = str(row['Affected Systems']).lower()\n",
        "        vuln_summary = str(row['Vulnerability Summary']).lower()\n",
        "\n",
        "        feature_dict = {\n",
        "            # Syst√®mes affect√©s\n",
        "            'has_windows': 1 if 'windows' in affected_systems else 0,\n",
        "            'has_linux': 1 if 'linux' in affected_systems else 0,\n",
        "            'has_macos': 1 if ('macos' in affected_systems or 'mac os' in affected_systems) else 0,\n",
        "            'has_android': 1 if 'android' in affected_systems else 0,\n",
        "            'has_ios': 1 if ('ios' in affected_systems or 'iphone' in affected_systems or 'ipad' in affected_systems) else 0,\n",
        "            'has_wordpress': 1 if 'wordpress' in affected_systems else 0,\n",
        "            'has_joomla': 1 if 'joomla' in affected_systems else 0,\n",
        "            'has_drupal': 1 if 'drupal' in affected_systems else 0,\n",
        "            'has_oracle': 1 if 'oracle' in affected_systems else 0,\n",
        "            'has_microsoft': 1 if 'microsoft' in affected_systems else 0,\n",
        "            'has_adobe': 1 if 'adobe' in affected_systems else 0,\n",
        "            'has_plugin': 1 if 'plugin' in affected_systems or 'extension' in affected_systems else 0,\n",
        "            'has_server': 1 if 'server' in affected_systems or 'serveur' in affected_systems else 0,\n",
        "            'has_web': 1 if 'web' in affected_systems else 0,\n",
        "            'has_mobile': 1 if 'mobile' in affected_systems else 0,\n",
        "            'has_iot': 1 if 'iot' in affected_systems or 'internet of things' in affected_systems else 0,\n",
        "            'has_cloud': 1 if 'cloud' in affected_systems or 'aws' in affected_systems or 'azure' in affected_systems else 0,\n",
        "\n",
        "            # Types de vuln√©rabilit√©s\n",
        "            'has_injection': 1 if 'injection' in vuln_summary else 0,\n",
        "            'has_xss': 1 if 'xss' in vuln_summary or 'cross-site' in vuln_summary or 'script' in vuln_summary else 0,\n",
        "            'has_sql': 1 if 'sql' in vuln_summary else 0,\n",
        "            'has_authentication': 1 if 'authentification' in vuln_summary or 'authentication' in vuln_summary else 0,\n",
        "            'has_authorization': 1 if 'autorisation' in vuln_summary or 'authorization' in vuln_summary else 0,\n",
        "            'has_privilege': 1 if 'privil√®ge' in vuln_summary or 'privilege' in vuln_summary else 0,\n",
        "            'has_code_execution': 1 if 'ex√©cution de code' in vuln_summary or 'code execution' in vuln_summary else 0,\n",
        "            'has_arbitrary_code': 1 if 'arbitraire' in vuln_summary or 'arbitrary' in vuln_summary else 0,\n",
        "            'has_buffer': 1 if 'buffer' in vuln_summary or 'tampon' in vuln_summary else 0,\n",
        "            'has_overflow': 1 if 'overflow' in vuln_summary or 'd√©bordement' in vuln_summary else 0,\n",
        "            'has_dos': 1 if 'd√©ni de service' in vuln_summary or 'denial of service' in vuln_summary or 'dos' in vuln_summary else 0,\n",
        "            'has_csrf': 1 if 'csrf' in vuln_summary or 'cross-site request' in vuln_summary else 0,\n",
        "            'has_path_traversal': 1 if 'path traversal' in vuln_summary or 'directory traversal' in vuln_summary else 0,\n",
        "            'has_file_inclusion': 1 if 'file inclusion' in vuln_summary or 'inclusion de fichier' in vuln_summary else 0,\n",
        "            'has_encryption': 1 if 'encryption' in vuln_summary or 'chiffrement' in vuln_summary else 0,\n",
        "            'has_ssl': 1 if 'ssl' in vuln_summary or 'tls' in vuln_summary else 0,\n",
        "            'has_mitm': 1 if 'man in the middle' in vuln_summary or 'mitm' in vuln_summary or 'homme du milieu' in vuln_summary else 0,\n",
        "            'has_bypass': 1 if 'bypass' in vuln_summary or 'contournement' in vuln_summary else 0,\n",
        "            'has_backdoor': 1 if 'backdoor' in vuln_summary or 'porte d√©rob√©e' in vuln_summary else 0,\n",
        "            'has_zero_day': 1 if 'zero day' in vuln_summary or 'zero-day' in vuln_summary or '0-day' in vuln_summary else 0,\n",
        "\n",
        "            # M√©triques existantes\n",
        "            'risk_level': row['Risk Level Numeric'],\n",
        "            'impact_level': row['Impact Level Numeric']\n",
        "        }\n",
        "\n",
        "        features.append(feature_dict)\n",
        "\n",
        "    # Conversion en DataFrame\n",
        "    X_features = pd.DataFrame(features)\n",
        "    print(f\"   ‚úì Caract√©ristiques bas√©es sur les r√®gles extraites: {X_features.shape[1]} dimensions\")\n",
        "\n",
        "    # Application du scaling\n",
        "    if self.config['scaling_method'] == 'standard':\n",
        "        self.scaler = StandardScaler()\n",
        "    elif self.config['scaling_method'] == 'minmax':\n",
        "        self.scaler = MinMaxScaler()\n",
        "    elif self.config['scaling_method'] == 'robust':\n",
        "        self.scaler = RobustScaler()\n",
        "    else:\n",
        "        self.scaler = StandardScaler()\n",
        "\n",
        "    X_features_scaled = self.scaler.fit_transform(X_features)\n",
        "    print(f\"   ‚úì Scaling appliqu√©: {self.config['scaling_method']}\")\n",
        "\n",
        "    # Enregistrement du scaler\n",
        "    joblib.dump(self.scaler, 'models/feature_scaler.pkl')\n",
        "\n",
        "    # S√©lection des caract√©ristiques si configur√©e\n",
        "    if self.config['feature_selection'] and X_features.shape[1] > self.config['n_features']:\n",
        "        print(f\"   - S√©lection des {self.config['n_features']} meilleures caract√©ristiques...\")\n",
        "\n",
        "        # Utilisation de SelectKBest avec f_classif\n",
        "        self.feature_selector = SelectKBest(f_classif, k=self.config['n_features'])\n",
        "        X_features_selected = self.feature_selector.fit_transform(X_features_scaled, df['Risk Level Numeric'])\n",
        "\n",
        "        # Enregistrement du s√©lecteur\n",
        "        joblib.dump(self.feature_selector, 'models/feature_selector.pkl')\n",
        "\n",
        "        print(f\"   ‚úì S√©lection de caract√©ristiques appliqu√©e: {X_features_selected.shape[1]} dimensions retenues\")\n",
        "\n",
        "        # R√©cup√©rer et afficher les noms des caract√©ristiques s√©lectionn√©es\n",
        "        selected_indices = self.feature_selector.get_support(indices=True)\n",
        "        selected_features = X_features.columns[selected_indices].tolist()\n",
        "        print(f\"   üìã Caract√©ristiques s√©lectionn√©es: {selected_features}\")\n",
        "\n",
        "        return X_features_selected, X_text, selected_features\n",
        "    else:\n",
        "        return X_features_scaled, X_text, X_features.columns.tolist()\n",
        "\n",
        "def apply_sampling(self, X, y):\n",
        "    \"\"\"Applique des techniques de sur-√©chantillonnage ou sous-√©chantillonnage\"\"\"\n",
        "    if not self.config['apply_smote']:\n",
        "        return X, y\n",
        "\n",
        "    print(\"\\n‚öñÔ∏è Application de techniques d'√©quilibrage des classes...\")\n",
        "\n",
        "    # Affichage de la distribution initiale\n",
        "    class_counts = pd.Series(y).value_counts().sort_index()\n",
        "    print(f\"   - Distribution initiale des classes: {class_counts.to_dict()}\")\n",
        "\n",
        "    # Application de SMOTE\n",
        "    smote = SMOTE(random_state=42)\n",
        "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "\n",
        "    # Affichage de la distribution apr√®s SMOTE\n",
        "    class_counts_after = pd.Series(y_resampled).value_counts().sort_index()\n",
        "    print(f\"   ‚úì Distribution apr√®s SMOTE: {class_counts_after.to_dict()}\")\n",
        "\n",
        "    return X_resampled, y_resampled"
      ],
      "metadata": {
        "id": "yOBjpLpT-BKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate(self, X, y, feature_names=None):\n",
        "    \"\"\"Entra√Æne et √©value tous les mod√®les configur√©s\"\"\"\n",
        "    print(\"\\nü§ñ Entra√Ænement et √©valuation des mod√®les...\")\n",
        "\n",
        "    # Initialisation des mod√®les\n",
        "    self.models = self.initialize_models()\n",
        "\n",
        "    if not self.models:\n",
        "        print(\"‚ö†Ô∏è Aucun mod√®le configur√© pour l'entra√Ænement\")\n",
        "        return\n",
        "\n",
        "    # Division des donn√©es\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "    print(f\"   - Donn√©es divis√©es: {X_train.shape[0]} exemples d'entra√Ænement, {X_test.shape[0]} exemples de test\")\n",
        "\n",
        "    # Entra√Ænement et √©valuation de chaque mod√®le\n",
        "    results = {}\n",
        "\n",
        "    for name, model in self.models.items():\n",
        "        start_time = time.time()\n",
        "        print(f\"\\n   üîÑ Entra√Ænement du mod√®le: {name}...\")\n",
        "\n",
        "        # V√©rification si Grid Search est activ√©\n",
        "        grid_search_config = self.models_config.get(name, {}).get('grid_search', False)\n",
        "        grid_params = self.models_config.get(name, {}).get('grid_params', {})\n",
        "\n",
        "        if grid_search_config and grid_params:\n",
        "            print(f\"      - Application de Grid Search avec {len(grid_params)} param√®tres\")\n",
        "            grid_search = GridSearchCV(\n",
        "                model,\n",
        "                grid_params,\n",
        "                cv=StratifiedKFold(n_splits=5),\n",
        "                scoring='f1_weighted',\n",
        "                n_jobs=-1\n",
        "            )\n",
        "            grid_search.fit(X_train, y_train)\n",
        "            model = grid_search.best_estimator_\n",
        "            print(f\"      ‚úì Meilleurs param√®tres: {grid_search.best_params_}\")\n",
        "        else:\n",
        "            model.fit(X_train, y_train)\n",
        "\n",
        "        # Pr√©dictions\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "        # Calcul des m√©triques\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "        # Temps d'entra√Ænement\n",
        "        training_time = time.time() - start_time\n",
        "\n",
        "        # Stockage des r√©sultats\n",
        "        results[name] = {\n",
        "            'model': model,\n",
        "            'accuracy': accuracy,\n",
        "            'f1_score': f1,\n",
        "            'training_time': training_time\n",
        "        }\n",
        "\n",
        "        # Affichage du rapport de classification\n",
        "        print(f\"      ‚úì Entra√Ænement termin√© en {training_time:.2f} secondes\")\n",
        "        print(f\"      ‚úì Accuracy: {accuracy:.4f}\")\n",
        "        print(f\"      ‚úì F1-Score: {f1:.4f}\")\n",
        "        print(\"\\n      üìä Rapport de classification:\")\n",
        "        print(classification_report(y_test, y_pred))\n",
        "\n",
        "        # Matrice de confusion\n",
        "        cm = confusion_matrix(y_test, y_pred)\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                    xticklabels=['Faible', 'Mod√©r√©', 'Important', 'Critique'],\n",
        "                    yticklabels=['Faible', 'Mod√©r√©', 'Important', 'Critique'])\n",
        "        plt.title(f'Matrice de confusion - {name}')\n",
        "        plt.ylabel('Valeur r√©elle')\n",
        "        plt.xlabel('Valeur pr√©dite')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'models/confusion_matrix_{name}.png')\n",
        "        plt.close()\n",
        "\n",
        "        # Importance des caract√©ristiques si disponible\n",
        "        if hasattr(model, 'feature_importances_') and feature_names is not None:\n",
        "            feature_importance = pd.DataFrame({\n",
        "                'Feature': feature_names,\n",
        "                'Importance': model.feature_importances_\n",
        "            }).sort_values('Importance', ascending=False)\n",
        "\n",
        "            # Ploter les 20 premi√®res caract√©ristiques les plus importantes\n",
        "            plt.figure(figsize=(10, 8))\n",
        "            sns.barplot(x='Importance', y='Feature', data=feature_importance.head(20))\n",
        "            plt.title(f'Importance des caract√©ristiques - {name}')\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(f'models/feature_importance_{name}.png')\n",
        "            plt.close()\n",
        "\n",
        "            # Sauvegarder l'importance des caract√©ristiques pour le meilleur mod√®le\n",
        "            if f1 > self.best_score:\n",
        "                self.feature_importance = feature_importance\n",
        "\n",
        "        # Sauvegarder le mod√®le\n",
        "        joblib.dump(model, f'models/{name}_model.pkl')\n",
        "\n",
        "        # Mise √† jour du meilleur mod√®le\n",
        "        if f1 > self.best_score:\n",
        "            self.best_score = f1\n",
        "            self.best_model_name = name\n",
        "            self.best_model = model\n",
        "\n",
        "    # Rapport final\n",
        "    print(\"\\nüìã R√©sum√© des performances des mod√®les:\")\n",
        "    results_df = pd.DataFrame({\n",
        "        'Mod√®le': list(results.keys()),\n",
        "        'Accuracy': [results[model]['accuracy'] for model in results],\n",
        "        'F1-Score': [results[model]['f1_score'] for model in results],\n",
        "        'Temps (s)': [results[model]['training_time'] for model in results]\n",
        "    }).sort_values('F1-Score', ascending=False)\n",
        "\n",
        "    print(results_df)\n",
        "\n",
        "    # Enregistrement du meilleur mod√®le\n",
        "    if self.best_model is not None:\n",
        "        print(f\"\\nüèÜ Meilleur mod√®le: {self.best_model_name} (F1-Score: {self.best_score:.4f})\")\n",
        "        joblib.dump(self.best_model, 'models/best_model.pkl')\n",
        "\n",
        "        # Sauvegarde du meilleur nom de mod√®le\n",
        "        with open('models/best_model_name.txt', 'w') as f:\n",
        "            f.write(self.best_model_name)\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "gHGc3FLw-hOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate(self, X, y, feature_names=None):\n",
        "    \"\"\"Entra√Æne et √©value tous les mod√®les configur√©s\"\"\"\n",
        "    print(\"\\nü§ñ Entra√Ænement et √©valuation des mod√®les...\")\n",
        "\n",
        "    # Initialisation des mod√®les\n",
        "    self.models = self.initialize_models()\n",
        "\n",
        "    if not self.models:\n",
        "        print(\"‚ö†Ô∏è Aucun mod√®le configur√© pour l'entra√Ænement\")\n",
        "        return\n",
        "\n",
        "    # Division des donn√©es\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "    print(f\"   - Donn√©es divis√©es: {X_train.shape[0]} exemples d'entra√Ænement, {X_test.shape[0]} exemples de test\")\n",
        "\n",
        "    # Entra√Ænement et √©valuation de chaque mod√®le\n",
        "    results = {}\n",
        "\n",
        "    for name, model in self.models.items():\n",
        "        start_time = time.time()\n",
        "        print(f\"\\n   üîÑ Entra√Ænement du mod√®le: {name}...\")\n",
        "\n",
        "        # V√©rification si Grid Search est activ√©\n",
        "        grid_search_config = self.models_config.get(name, {}).get('grid_search', False)\n",
        "        grid_params = self.models_config.get(name, {}).get('grid_params', {})\n",
        "\n",
        "        if grid_search_config and grid_params:\n",
        "            print(f\"      - Application de Grid Search avec {len(grid_params)} param√®tres\")\n",
        "            grid_search = GridSearchCV(\n",
        "                model,\n",
        "                grid_params,\n",
        "                cv=StratifiedKFold(n_splits=5),\n",
        "                scoring='f1_weighted',\n",
        "                n_jobs=-1\n",
        "            )\n",
        "            grid_search.fit(X_train, y_train)\n",
        "            model = grid_search.best_estimator_\n",
        "            print(f\"      ‚úì Meilleurs param√®tres: {grid_search.best_params_}\")\n",
        "        else:\n",
        "            model.fit(X_train, y_train)\n",
        "\n",
        "        # Pr√©dictions\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "        # Calcul des m√©triques\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "        # Temps d'entra√Ænement\n",
        "        training_time = time.time() - start_time\n",
        "\n",
        "        # Stockage des r√©sultats\n",
        "        results[name] = {\n",
        "            'model': model,\n",
        "            'accuracy': accuracy,\n",
        "            'f1_score': f1,\n",
        "            'training_time': training_time\n",
        "        }\n",
        "\n",
        "        # Affichage du rapport de classification\n",
        "        print(f\"      ‚úì Entra√Ænement termin√© en {training_time:.2f} secondes\")\n",
        "        print(f\"      ‚úì Accuracy: {accuracy:.4f}\")\n",
        "        print(f\"      ‚úì F1-Score: {f1:.4f}\")\n",
        "        print(\"\\n      üìä Rapport de classification:\")\n",
        "        print(classification_report(y_test, y_pred))\n",
        "\n",
        "        # Matrice de confusion\n",
        "        cm = confusion_matrix(y_test, y_pred)\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                    xticklabels=['Faible', 'Mod√©r√©', 'Important', 'Critique'],\n",
        "                    yticklabels=['Faible', 'Mod√©r√©', 'Important', 'Critique'])\n",
        "        plt.title(f'Matrice de confusion - {name}')\n",
        "        plt.ylabel('Valeur r√©elle')\n",
        "        plt.xlabel('Valeur pr√©dite')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'models/confusion_matrix_{name}.png')\n",
        "        plt.close()\n",
        "\n",
        "        # Importance des caract√©ristiques si disponible\n",
        "        if hasattr(model, 'feature_importances_') and feature_names is not None:\n",
        "            feature_importance = pd.DataFrame({\n",
        "                'Feature': feature_names,\n",
        "                'Importance': model.feature_importances_\n",
        "            }).sort_values('Importance', ascending=False)\n",
        "\n",
        "            # Ploter les 20 premi√®res caract√©ristiques les plus importantes\n",
        "            plt.figure(figsize=(10, 8))\n",
        "            sns.barplot(x='Importance', y='Feature', data=feature_importance.head(20))\n",
        "            plt.title(f'Importance des caract√©ristiques - {name}')\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(f'models/feature_importance_{name}.png')\n",
        "            plt.close()\n",
        "\n",
        "            # Sauvegarder l'importance des caract√©ristiques pour le meilleur mod√®le\n",
        "            if f1 > self.best_score:\n",
        "                self.feature_importance = feature_importance\n",
        "\n",
        "        # Sauvegarder le mod√®le\n",
        "        joblib.dump(model, f'models/{name}_model.pkl')\n",
        "\n",
        "        # Mise √† jour du meilleur mod√®le\n",
        "        if f1 > self.best_score:\n",
        "            self.best_score = f1\n",
        "            self.best_model_name = name\n",
        "            self.best_model = model\n",
        "\n",
        "    # Rapport final\n",
        "    print(\"\\nüìã R√©sum√© des performances des mod√®les:\")\n",
        "    results_df = pd.DataFrame({\n",
        "        'Mod√®le': list(results.keys()),\n",
        "        'Accuracy': [results[model]['accuracy'] for model in results],\n",
        "        'F1-Score': [results[model]['f1_score'] for model in results],\n",
        "        'Temps (s)': [results[model]['training_time'] for model in results]\n",
        "    }).sort_values('F1-Score', ascending=False)\n",
        "\n",
        "    print(results_df)\n",
        "\n",
        "    # Enregistrement du meilleur mod√®le\n",
        "    if self.best_model is not None:\n",
        "        print(f\"\\nüèÜ Meilleur mod√®le: {self.best_model_name} (F1-Score: {self.best_score:.4f})\")\n",
        "        joblib.dump(self.best_model, 'models/best_model.pkl')\n",
        "\n",
        "        # Sauvegarde du meilleur nom de mod√®le\n",
        "        with open('models/best_model_name.txt', 'w') as f:\n",
        "            f.write(self.best_model_name)\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "LrBSXJFu-ky0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SECTION 2: ENTRA√éNEMENT DE MULTIPLES MOD√àLES\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "PDmhRouSaTjq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiModelTrainer:\n",
        "    \"\"\"Classe pour l'entra√Ænement et l'√©valuation de multiples mod√®les\"\"\"\n",
        "\n",
        "    def __init__(self, models_config=None):\n",
        "        self.models_config = models_config or {\n",
        "            'random_forest': {\n",
        "                'enabled': True,\n",
        "                'params': {\n",
        "                    'n_estimators': 100,\n",
        "                    'max_depth': None,\n",
        "                    'min_samples_split': 2,\n",
        "                    'random_state': 42\n",
        "                },\n",
        "                'grid_search': False,\n",
        "                'grid_params': {\n",
        "                    'n_estimators': [50, 100, 200],\n",
        "                    'max_depth': [None, 10, 20],\n",
        "                    'min_samples_split': [2, 5, 10]\n",
        "                }\n",
        "            },\n",
        "            'xgboost': {\n",
        "                'enabled': True,\n",
        "                'params': {\n",
        "                    'n_estimators': 100,\n",
        "                    'learning_rate': 0.1,\n",
        "                    'max_depth': 5,\n",
        "                    'random_state': 42\n",
        "                },\n",
        "                'grid_search': False,\n",
        "                'grid_params': {\n",
        "                    'n_estimators': [50, 100, 200],\n",
        "                    'learning_rate': [0.01, 0.1, 0.2],\n",
        "                    'max_depth': [3, 5, 7]\n",
        "                }\n",
        "            },\n",
        "            'gradient_boosting': {\n",
        "                'enabled': True,\n",
        "                'params': {\n",
        "                    'n_estimators': 100,\n",
        "                    'learning_rate': 0.1,\n",
        "                    'max_depth': 3,\n",
        "                    'random_state': 42\n",
        "                },\n",
        "                'grid_search': False,\n",
        "                'grid_params': {\n",
        "                    'n_estimators': [50, 100, 200],\n",
        "                    'learning_rate': [0.01, 0.1, 0.2],\n",
        "                    'max_depth': [3, 5, 7]\n",
        "                }\n",
        "            },\n",
        "            'logistic_regression': {\n",
        "                'enabled': True,\n",
        "                'params': {\n",
        "                    'C': 1.0,\n",
        "                    'random_state': 42\n",
        "                },\n",
        "                'grid_search': False\n",
        "            },\n",
        "            'voting_ensemble': {\n",
        "                'enabled': True,\n",
        "                'estimators': ['random_forest', 'xgboost', 'gradient_boosting'],\n",
        "                'voting': 'soft'\n",
        "            }\n",
        "        }\n",
        "\n",
        "        self.models = {}\n",
        "        self.best_model_name = None\n",
        "        self.best_model = None\n",
        "        self.best_score = 0\n",
        "        self.feature_importance = None\n",
        "\n",
        "    def initialize_models(self):\n",
        "        \"\"\"Initialise tous les mod√®les configur√©s\"\"\"\n",
        "        models = {}\n",
        "\n",
        "        # Random Forest\n",
        "        if self.models_config['random_forest']['enabled']:\n",
        "            models['random_forest'] = RandomForestClassifier(**self.models_config['random_forest']['params'])\n",
        "\n",
        "        # XGBoost\n",
        "        if self.models_config['xgboost']['enabled']:\n",
        "            models['xgboost'] = XGBClassifier(**self.models_config['xgboost']['params'])\n",
        "\n",
        "        # Gradient Boosting\n",
        "        if self.models_config['gradient_boosting']['enabled']:\n",
        "            models['gradient_boosting'] = GradientBoostingClassifier(**self.models_config['gradient_boosting']['params'])\n",
        "\n",
        "        # Logistic Regression\n",
        "        if self.models_config['logistic_regression']['enabled']:\n",
        "            models['logistic_regression'] = LogisticRegression(**self.models_config['logistic_regression']['params'])\n",
        "\n",
        "        # Ajout du mod√®le d'ensemble si configur√©\n",
        "        if self.models_config['voting_ensemble']['enabled']:\n",
        "            estimators = []\n",
        "            for est_name in self.models_config['voting_ensemble']['estimators']:\n",
        "                if est_name in models:\n",
        "                    estimators.append((est_name, models[est_name]))\n",
        "\n",
        "            if estimators:\n",
        "                models['voting_ensemble'] = VotingClassifier(\n",
        "                    estimators=estimators,\n",
        "                    voting=self.models_config['voting_ensemble']['voting']\n",
        "                )\n",
        "\n",
        "        return models\n",
        "\n",
        "    def train_and_evaluate(self, X, y, feature_names=None):\n",
        "        \"\"\"Entra√Æne et √©value tous les mod√®les configur√©s\"\"\"\n",
        "        print(\"\\nü§ñ Entra√Ænement et √©valuation des mod√®les...\")\n",
        "\n",
        "        # Initialisation des mod√®les\n",
        "        self.models = self.initialize_models()\n",
        "\n",
        "        if not self.models:\n",
        "            print(\"‚ö†Ô∏è Aucun mod√®le configur√© pour l'entra√Ænement\")\n",
        "            return\n",
        "\n",
        "        # Division des donn√©es\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "        print(f\"   - Donn√©es divis√©es: {X_train.shape[0]} exemples d'entra√Ænement, {X_test.shape[0]} exemples de test\")\n",
        "\n",
        "        # Entra√Ænement et √©valuation de chaque mod√®le\n",
        "        results = {}\n",
        "\n",
        "        for name, model in self.models.items():\n",
        "            start_time = time.time()\n",
        "            print(f\"\\n   üîÑ Entra√Ænement du mod√®le: {name}...\")\n",
        "\n",
        "            # V√©rification si Grid Search est activ√©\n",
        "            grid_search_config = self.models_config.get(name, {}).get('grid_search', False)\n",
        "            grid_params = self.models_config.get(name, {}).get('grid_params', {})\n",
        "\n",
        "            if grid_search_config and grid_params:\n",
        "                print(f\"      - Application de Grid Search avec {len(grid_params)} param√®tres\")\n",
        "                grid_search = GridSearchCV(\n",
        "                    model,\n",
        "                    grid_params,\n",
        "                    cv=StratifiedKFold(n_splits=5),\n",
        "                    scoring='f1_weighted',\n",
        "                    n_jobs=-1\n",
        "                )\n",
        "                grid_search.fit(X_train, y_train)\n",
        "                model = grid_search.best_estimator_\n",
        "                print(f\"      ‚úì Meilleurs param√®tres: {grid_search.best_params_}\")\n",
        "            else:\n",
        "                model.fit(X_train, y_train)\n",
        "\n",
        "            # Pr√©dictions\n",
        "            y_pred = model.predict(X_test)\n",
        "\n",
        "            # Calcul des m√©triques\n",
        "            accuracy = accuracy_score(y_test, y_pred)\n",
        "            f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "            # Temps d'entra√Ænement\n",
        "            training_time = time.time() - start_time\n",
        "\n",
        "            # Stockage des r√©sultats\n",
        "            results[name] = {\n",
        "                'model': model,\n",
        "                'accuracy': accuracy,\n",
        "                'f1_score': f1,\n",
        "                'training_time': training_time\n",
        "            }\n",
        "\n",
        "            # Affichage du rapport de classification\n",
        "            print(f\"      ‚úì Entra√Ænement termin√© en {training_time:.2f} secondes\")\n",
        "            print(f\"      ‚úì Accuracy: {accuracy:.4f}\")\n",
        "            print(f\"      ‚úì F1-Score: {f1:.4f}\")\n",
        "            print(\"\\n      üìä Rapport de classification:\")\n",
        "            print(classification_report(y_test, y_pred))\n",
        "\n",
        "            # Matrice de confusion\n",
        "            cm = confusion_matrix(y_test, y_pred)\n",
        "            plt.figure(figsize=(8, 6))\n",
        "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                        xticklabels=['Faible', 'Mod√©r√©', 'Important', 'Critique'],\n",
        "                        yticklabels=['Faible', 'Mod√©r√©', 'Important', 'Critique'])\n",
        "            plt.title(f'Matrice de confusion - {name}')\n",
        "            plt.ylabel('Valeur r√©elle')\n",
        "            plt.xlabel('Valeur pr√©dite')\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(f'models/confusion_matrix_{name}.png')\n",
        "            plt.close()\n",
        "\n",
        "            # Importance des caract√©ristiques si disponible\n",
        "            if hasattr(model, 'feature_importances_') and feature_names is not None:\n",
        "                feature_importance = pd.DataFrame({\n",
        "                    'Feature': feature_names,\n",
        "                    'Importance': model.feature_importances_\n",
        "                }).sort_values('Importance', ascending=False)\n",
        "\n",
        "                # Ploter les 20 premi√®res caract√©ristiques les plus importantes\n",
        "                plt.figure(figsize=(10, 8))\n",
        "                sns.barplot(x='Importance', y='Feature', data=feature_importance.head(20))\n",
        "                plt.title(f'Importance des caract√©ristiques - {name}')\n",
        "                plt.tight_layout()\n",
        "                plt.savefig(f'models/feature_importance_{name}.png')\n",
        "                plt.close()\n",
        "\n",
        "                # Sauvegarder l'importance des caract√©ristiques pour le meilleur mod√®le\n",
        "                if f1 > self.best_score:\n",
        "                    self.feature_importance = feature_importance\n",
        "\n",
        "            # Sauvegarder le mod√®le\n",
        "            joblib.dump(model, f'models/{name}_model.pkl')\n",
        "\n",
        "            # Mise √† jour du meilleur mod√®le\n",
        "            if f1 > self.best_score:\n",
        "                self.best_score = f1\n",
        "                self.best_model_name = name\n",
        "                self.best_model = model\n",
        "\n",
        "        # Rapport final\n",
        "        print(\"\\nüìã R√©sum√© des performances des mod√®les:\")\n",
        "        results_df = pd.DataFrame({\n",
        "            'Mod√®le': list(results.keys()),\n",
        "            'Accuracy': [results[model]['accuracy'] for model in results],\n",
        "            'F1-Score': [results[model]['f1_score'] for model in results],\n",
        "            'Temps (s)': [results[model]['training_time'] for model in results]\n",
        "        }).sort_values('F1-Score', ascending=False)\n",
        "\n",
        "        print(results_df)\n",
        "\n",
        "        # Enregistrement du meilleur mod√®le\n",
        "        if self.best_model is not None:\n",
        "            print(f\"\\nüèÜ Meilleur mod√®le: {self.best_model_name} (F1-Score: {self.best_score:.4f})\")\n",
        "            joblib.dump(self.best_model, 'models/best_model.pkl')\n",
        "\n",
        "            # Sauvegarde du meilleur nom de mod√®le\n",
        "            with open('models/best_model_name.txt', 'w') as f:\n",
        "                f.write(self.best_model_name)\n",
        "\n",
        "        return results"
      ],
      "metadata": {
        "id": "H_GstuVQaGYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SECTION 3: D√âTECTION D'ANOMALIES\n"
      ],
      "metadata": {
        "id": "huLkfZhSbHV0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.neighbors import LocalOutlierFactor  # Attention: c'est dans sklearn.neighbors, pas sklearn.ensemble\n",
        "from sklearn.decomposition import PCA"
      ],
      "metadata": {
        "id": "i4RfhbTTaoBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AnomalyDetector:\n",
        "    \"\"\"Classe pour la d√©tection d'anomalies et l'identification de vuln√©rabilit√©s rares\"\"\"\n",
        "\n",
        "    def __init__(self, config=None):\n",
        "        self.config = config or {\n",
        "            'isolation_forest': {\n",
        "                'enabled': True,\n",
        "                'params': {\n",
        "                    'n_estimators': 100,\n",
        "                    'contamination': 0.05,\n",
        "                    'random_state': 42\n",
        "                }\n",
        "            },\n",
        "            'local_outlier_factor': {\n",
        "                'enabled': True,\n",
        "                'params': {\n",
        "                    'n_neighbors': 20,\n",
        "                    'contamination': 0.05\n",
        "                }\n",
        "            },\n",
        "            'one_class_svm': {\n",
        "                'enabled': False,\n",
        "                'params': {\n",
        "                    'kernel': 'rbf',\n",
        "                    'nu': 0.05,\n",
        "                    'gamma': 'scale'\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "\n",
        "        self.models = {}\n",
        "        self.best_model = None\n",
        "        self.best_model_name = None\n",
        "\n",
        "    def train_anomaly_models(self, X):\n",
        "        \"\"\"Entra√Æne les mod√®les de d√©tection d'anomalies\"\"\"\n",
        "        print(\"\\nüîç Entra√Ænement des mod√®les de d√©tection d'anomalies...\")\n",
        "\n",
        "    # Isolation Forest\n",
        "        if 'isolation_forest' in self.config and self.config['isolation_forest']['enabled']:\n",
        "            print(\"   - Entra√Ænement d'Isolation Forest...\")\n",
        "            iso_forest = IsolationForest(**self.config['isolation_forest']['params'])\n",
        "            iso_forest.fit(X)\n",
        "            self.models['isolation_forest'] = iso_forest\n",
        "            joblib.dump(iso_forest, 'models/isolation_forest_model.pkl')\n",
        "            print(\"   ‚úì Mod√®le Isolation Forest enregistr√©\")\n",
        "\n",
        "        # D√©finir comme meilleur mod√®le par d√©faut si c'est le premier\n",
        "        if self.best_model is None:\n",
        "            self.best_model = iso_forest\n",
        "            self.best_model_name = 'isolation_forest'\n",
        "\n",
        "    # Local Outlier Factor\n",
        "        if 'local_outlier_factor' in self.config and self.config['local_outlier_factor']['enabled']:\n",
        "            print(\"   - Entra√Ænement de Local Outlier Factor...\")\n",
        "            lof = LocalOutlierFactor(**self.config['local_outlier_factor']['params'])\n",
        "        # LOF a besoin d'√™tre ajust√© et ne peut pas √™tre utilis√© directement pour pr√©dire de nouvelles donn√©es\n",
        "        # Nous allons donc l'entra√Æner sur les donn√©es d'entra√Ænement et sauvegarder le mod√®le\n",
        "            lof.fit_predict(X)\n",
        "            self.models['local_outlier_factor'] = lof\n",
        "            joblib.dump(lof, 'models/local_outlier_factor_model.pkl')\n",
        "            print(\"   ‚úì Mod√®le Local Outlier Factor enregistr√©\")\n",
        "\n",
        "    # One-Class SVM - v√©rifier si la cl√© existe\n",
        "        if 'one_class_svm' in self.config and self.config['one_class_svm']['enabled']:\n",
        "            try:\n",
        "               from sklearn.svm import OneClassSVM\n",
        "               print(\"   - Entra√Ænement de One-Class SVM...\")\n",
        "               ocsvm = OneClassSVM(**self.config['one_class_svm']['params'])\n",
        "               ocsvm.fit(X)\n",
        "               self.models['one_class_svm'] = ocsvm\n",
        "               joblib.dump(ocsvm, 'models/one_class_svm_model.pkl')\n",
        "               print(\"   ‚úì Mod√®le One-Class SVM enregistr√©\")\n",
        "            except Exception as e:\n",
        "               print(f\"   ‚ö†Ô∏è Erreur lors de l'entra√Ænement de One-Class SVM: {e}\")\n",
        "\n",
        "    # Enregistrement du meilleur mod√®le (par d√©faut, Isolation Forest)\n",
        "        if self.best_model is not None:\n",
        "          joblib.dump(self.best_model, 'models/best_anomaly_model.pkl')\n",
        "          with open('models/best_anomaly_model_name.txt', 'w') as f:\n",
        "               f.write(self.best_model_name)\n",
        "\n",
        "        return self.models\n",
        "\n",
        "    def visualize_anomaly_detection(self, X, model_name='isolation_forest'):\n",
        "        \"\"\"Visualise les r√©sultats de la d√©tection d'anomalies\"\"\"\n",
        "        if model_name not in self.models:\n",
        "            print(f\"‚ö†Ô∏è Mod√®le {model_name} non trouv√©.\")\n",
        "            return\n",
        "\n",
        "        print(f\"\\nüìä Visualisation des anomalies d√©tect√©es par {model_name}...\")\n",
        "\n",
        "        # R√©duire la dimensionnalit√© pour visualisation\n",
        "        pca = PCA(n_components=2)\n",
        "        X_2d = pca.fit_transform(X)\n",
        "\n",
        "        # Pr√©diction des anomalies\n",
        "        if model_name == 'isolation_forest':\n",
        "            y_pred = self.models[model_name].predict(X)\n",
        "        elif model_name == 'local_outlier_factor':\n",
        "            # Pour LOF, nous utilisons les scores de nouveaut√©\n",
        "            y_pred = self.models[model_name].fit_predict(X)\n",
        "        else:\n",
        "            y_pred = self.models[model_name].predict(X)\n",
        "\n",
        "        # Conversion des valeurs pr√©dites (-1 pour anomalie, 1 pour normal)\n",
        "        anomalies = y_pred == -1\n",
        "\n",
        "        # Cr√©ation d'un dataframe pour visualisation\n",
        "        df_viz = pd.DataFrame({\n",
        "            'PC1': X_2d[:, 0],\n",
        "            'PC2': X_2d[:, 1],\n",
        "            'Anomalie': anomalies\n",
        "        })\n",
        "\n",
        "        # Calcul du pourcentage d'anomalies\n",
        "        anomaly_percentage = (anomalies.sum() / len(anomalies)) * 100\n",
        "\n",
        "        # Visualisation\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        sns.scatterplot(x='PC1', y='PC2', hue='Anomalie', data=df_viz, palette={False: 'blue', True: 'red'})\n",
        "        plt.title(f'D√©tection d\\'anomalies - {model_name}\\n{anomalies.sum()} anomalies d√©tect√©es ({anomaly_percentage:.2f}%)')\n",
        "        plt.legend(['Normal', 'Anomalie'])\n",
        "        plt.savefig(f'models/anomaly_detection_{model_name}.png')\n",
        "        plt.close()\n",
        "\n",
        "        print(f\"   ‚úì {anomalies.sum()} anomalies d√©tect√©es ({anomaly_percentage:.2f}%)\")\n",
        "        return df_viz\n",
        "\n",
        "\n",
        "\n",
        "def create_directory_structure():\n",
        "    \"\"\"Cr√©e la structure de r√©pertoires n√©cessaire pour le projet\"\"\"\n",
        "    print(\"\\nüìÇ Cr√©ation de la structure de r√©pertoires...\")\n",
        "\n",
        "    # Liste des r√©pertoires √† cr√©er\n",
        "    directories = [\n",
        "        'models',\n",
        "        'data',\n",
        "        'reports',\n",
        "        'logs',\n",
        "        'static',\n",
        "        'static/img',\n",
        "        'templates'\n",
        "    ]\n",
        "\n",
        "    # Cr√©ation des r√©pertoires\n",
        "    for directory in directories:\n",
        "        os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "    print(\"   ‚úì Structure de r√©pertoires cr√©√©e\")\n",
        "\n",
        "def generate_security_report(result, output_format='json'):\n",
        "    \"\"\"G√©n√®re un rapport de s√©curit√© dans le format sp√©cifi√©\"\"\"\n",
        "    print(\"\\nüìù G√©n√©ration du rapport de s√©curit√©...\")\n",
        "\n",
        "    if output_format == 'json':\n",
        "        # Enregistrement du rapport au format JSON\n",
        "        report_file = f\"reports/security_report_{result['url'].replace('://', '_').replace('/', '_').replace('.', '_')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
        "        with open(report_file, 'w') as f:\n",
        "            json.dump(result, f, indent=4)\n",
        "        print(f\"   ‚úì Rapport JSON enregistr√©: {report_file}\")\n",
        "        return report_file\n",
        "    elif output_format == 'html':\n",
        "        # Cr√©ation d'un rapport HTML simple\n",
        "        report_file = f\"reports/security_report_{result['url'].replace('://', '_').replace('/', '_').replace('.', '_')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html\"\n",
        "\n",
        "        # Template HTML basique\n",
        "        html_template = f\"\"\"\n",
        "        <!DOCTYPE html>\n",
        "        <html>\n",
        "        <head>\n",
        "            <title>Rapport de s√©curit√© - {result['url']}</title>\n",
        "            <style>\n",
        "                body {{ font-family: Arial, sans-serif; margin: 20px; }}\n",
        "                h1, h2, h3 {{ color: #333; }}\n",
        "                .header {{ background-color: #f8f9fa; padding: 20px; border-radius: 5px; margin-bottom: 20px; }}\n",
        "                .risk-score {{ font-size: 24px; font-weight: bold; }}\n",
        "                .risk-critical {{ color: #dc3545; }}\n",
        "                .risk-important {{ color: #fd7e14; }}\n",
        "                .risk-moderate {{ color: #ffc107; }}\n",
        "                .risk-low {{ color: #28a745; }}\n",
        "                .vulnerability {{ margin: 10px 0; padding: 15px; border-radius: 5px; }}\n",
        "                .vulnerability-critical {{ background-color: #f8d7da; border-left: 5px solid #dc3545; }}\n",
        "                .vulnerability-important {{ background-color: #fff3cd; border-left: 5px solid #fd7e14; }}\n",
        "                .vulnerability-moderate {{ background-color: #fff3cd; border-left: 5px solid #ffc107; }}\n",
        "                .recommendation {{ margin: 10px 0; padding: 15px; background-color: #e9ecef; border-radius: 5px; }}\n",
        "            </style>\n",
        "        </head>\n",
        "        <body>\n",
        "            <div class=\"header\">\n",
        "                <h1>Rapport de s√©curit√© web</h1>\n",
        "                <p><strong>URL analys√©e:</strong> {result['url']}</p>\n",
        "                <p><strong>Date d'analyse:</strong> {result['scan_date']}</p>\n",
        "                <p class=\"risk-score {\n",
        "                    'risk-critical' if result['risk_level_text'] == 'Critique' else\n",
        "                    'risk-important' if result['risk_level_text'] == 'Important' else\n",
        "                    'risk-moderate' if result['risk_level_text'] == 'Mod√©r√©' else\n",
        "                    'risk-low'\n",
        "                }\">\n",
        "                    Niveau de risque: {result['risk_level_text']} ({result['risk_score']}%)\n",
        "                </p>\n",
        "                <p><strong>Anomalie d√©tect√©e:</strong> {'Oui' if result['is_anomaly'] else 'Non'}</p>\n",
        "            </div>\n",
        "\n",
        "            <h2>Vuln√©rabilit√©s d√©tect√©es ({result['vulnerability_count']})</h2>\n",
        "        \"\"\"\n",
        "\n",
        "        # Ajout des vuln√©rabilit√©s\n",
        "        if result['vulnerability_count'] > 0:\n",
        "            for vuln in result['vulnerabilities']:\n",
        "                severity_class = 'vulnerability-critical' if vuln['severity'] == 'Critique' else \\\n",
        "                                'vulnerability-important' if vuln['severity'] == 'Important' else \\\n",
        "                                'vulnerability-moderate'\n",
        "                html_template += f\"\"\"\n",
        "                <div class=\"vulnerability {severity_class}\">\n",
        "                    <h3>{vuln['name']}</h3>\n",
        "                    <p>{vuln['description']}</p>\n",
        "                    <p><strong>S√©v√©rit√©:</strong> {vuln['severity']}</p>\n",
        "                </div>\n",
        "                \"\"\"\n",
        "        else:\n",
        "            html_template += \"<p>Aucune vuln√©rabilit√© d√©tect√©e.</p>\"\n",
        "\n",
        "        # Ajout des recommandations\n",
        "        html_template += \"<h2>Recommandations de s√©curit√©</h2>\"\n",
        "\n",
        "        if result['recommendations']:\n",
        "            for rec in result['recommendations']:\n",
        "                html_template += f\"\"\"\n",
        "                <div class=\"recommendation\">\n",
        "                    <h3>{rec['title']}</h3>\n",
        "                    <p>{rec['description']}</p>\n",
        "                    <p><strong>Priorit√©:</strong> {rec['priority']}</p>\n",
        "                </div>\n",
        "                \"\"\"\n",
        "        else:\n",
        "            html_template += \"<p>Aucune recommandation sp√©cifique.</p>\"\n",
        "\n",
        "        # Ajout des probabilit√©s\n",
        "        html_template += \"\"\"\n",
        "            <h2>Probabilit√©s par niveau de risque</h2>\n",
        "            <table border=\"1\" cellpadding=\"5\" cellspacing=\"0\">\n",
        "                <tr>\n",
        "                    <th>Niveau</th>\n",
        "                    <th>Probabilit√©</th>\n",
        "                </tr>\n",
        "        \"\"\"\n",
        "\n",
        "        for level, prob in result['probability'].items():\n",
        "            html_template += f\"\"\"\n",
        "                <tr>\n",
        "                    <td>{level}</td>\n",
        "                    <td>{prob:.2f}</td>\n",
        "                </tr>\n",
        "            \"\"\"\n",
        "\n",
        "        html_template += \"\"\"\n",
        "            </table>\n",
        "\n",
        "            <div style=\"margin-top: 30px; text-align: center; color: #6c757d;\">\n",
        "                <p>Rapport g√©n√©r√© par le syst√®me de pr√©diction de vuln√©rabilit√©s web.</p>\n",
        "                <p>¬© 2025 Direction G√©n√©rale de la S√©curit√© des Syst√®mes d'Information</p>\n",
        "            </div>\n",
        "        </body>\n",
        "        </html>\n",
        "        \"\"\"\n",
        "\n",
        "        # Enregistrement du rapport HTML\n",
        "        with open(report_file, 'w') as f:\n",
        "            f.write(html_template)\n",
        "\n",
        "        print(f\"   ‚úì Rapport HTML enregistr√©: {report_file}\")\n",
        "        return report_file\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è Format de rapport non pris en charge: {output_format}\")\n",
        "        return None\n",
        "\n",
        "def train_complete_pipeline(data_file, config=None):\n",
        "    \"\"\"Ex√©cute le pipeline complet d'entra√Ænement des mod√®les\"\"\"\n",
        "    # Configuration par d√©faut\n",
        "    default_config = {\n",
        "        'preprocessing': {\n",
        "            'text_cleaning': True,\n",
        "            'handle_missing': 'fill_empty',\n",
        "            'scaling_method': 'standard',\n",
        "            'feature_selection': True,\n",
        "            'n_features': 20,\n",
        "            'stemming': True,\n",
        "            'language': 'french',\n",
        "            'apply_smote': False\n",
        "        },\n",
        "        'models': {\n",
        "            'random_forest': {\n",
        "                'enabled': True,\n",
        "                'params': {'n_estimators': 100, 'random_state': 42},\n",
        "                'grid_search': False\n",
        "            },\n",
        "            'xgboost': {\n",
        "                'enabled': True,\n",
        "                'params': {'n_estimators': 100, 'learning_rate': 0.1, 'random_state': 42},\n",
        "                'grid_search': False\n",
        "            },\n",
        "            'gradient_boosting': {\n",
        "                'enabled': True,\n",
        "                'params': {'n_estimators': 100, 'learning_rate': 0.1, 'random_state': 42},\n",
        "                'grid_search': False\n",
        "            },\n",
        "            'logistic_regression': {\n",
        "                'enabled': True,\n",
        "                'params': {'C': 1.0, 'random_state': 42},\n",
        "                'grid_search': False\n",
        "            },\n",
        "            'voting_ensemble': {\n",
        "                'enabled': True,\n",
        "                'estimators': ['random_forest', 'xgboost', 'gradient_boosting'],\n",
        "                'voting': 'soft'\n",
        "            }\n",
        "        },\n",
        "        'anomaly_detection': {\n",
        "            'isolation_forest': {\n",
        "                'enabled': True,\n",
        "                'params': {'n_estimators': 100, 'contamination': 0.05, 'random_state': 42}\n",
        "            },\n",
        "            'local_outlier_factor': {\n",
        "                'enabled': True,\n",
        "                'params': {'n_neighbors': 20, 'contamination': 0.05}\n",
        "            }\n",
        "        },\n",
        "        'deep_learning': {\n",
        "            'enabled': False,\n",
        "            'model_type': 'mlp',\n",
        "            'dense_units': [128, 64],\n",
        "            'dropout_rate': 0.3,\n",
        "            'learning_rate': 0.001,\n",
        "            'batch_size': 32,\n",
        "            'epochs': 50\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Fusion de la configuration personnalis√©e avec la configuration par d√©faut\n",
        "    if config:\n",
        "        # Fonction r√©cursive pour fusionner des dictionnaires imbriqu√©s\n",
        "        def merge_dicts(d1, d2):\n",
        "            for k, v in d2.items():\n",
        "                if k in d1 and isinstance(d1[k], dict) and isinstance(v, dict):\n",
        "                    merge_dicts(d1[k], v)\n",
        "                else:\n",
        "                    d1[k] = v\n",
        "            return d1\n",
        "\n",
        "        config = merge_dicts(default_config.copy(), config)\n",
        "    else:\n",
        "        config = default_config\n",
        "\n",
        "    # Cr√©ation de la structure de r√©pertoires\n",
        "    create_directory_structure()\n",
        "\n",
        "    # Pr√©traitement des donn√©es\n",
        "    preprocessor = DataPreprocessor(config['preprocessing'])\n",
        "    df = preprocessor.load_and_preprocess(data_file)\n",
        "\n",
        "    # Extraction des caract√©ristiques\n",
        "    X_features, X_text, selected_features = preprocessor.extract_features(df)\n",
        "\n",
        "    # Pr√©paration des donn√©es pour l'entra√Ænement\n",
        "    X = X_features  # Nous utilisons uniquement les caract√©ristiques bas√©es sur les r√®gles pour simplifier\n",
        "    y = df['Risk Level Numeric'].values\n",
        "\n",
        "    # Application de techniques de r√©√©chantillonnage si configur√©\n",
        "    if config['preprocessing']['apply_smote']:\n",
        "        X, y = preprocessor.apply_sampling(X, y)\n",
        "\n",
        "    # Entra√Ænement des mod√®les de classification\n",
        "    model_trainer = MultiModelTrainer(config['models'])\n",
        "    model_results = model_trainer.train_and_evaluate(X, y, selected_features)\n",
        "\n",
        "    # Entra√Ænement des mod√®les de d√©tection d'anomalies\n",
        "    anomaly_detector = AnomalyDetector(config['anomaly_detection'])\n",
        "    anomaly_models = anomaly_detector.train_anomaly_models(X)\n",
        "\n",
        "    # Visualisation des r√©sultats de d√©tection d'anomalies\n",
        "    anomaly_viz = anomaly_detector.visualize_anomaly_detection(X)\n",
        "\n",
        "    # Entra√Ænement du mod√®le de deep learning si activ√©\n",
        "    if config['deep_learning']['enabled']:\n",
        "        dl_classifier = DeepLearningClassifier(config['deep_learning'])\n",
        "        dl_history = dl_classifier.train(X, y)\n",
        "\n",
        "    print(\"\\n‚úÖ Pipeline d'entra√Ænement complet termin√©!\")\n",
        "    return {\n",
        "        'preprocessor': preprocessor,\n",
        "        'model_trainer': model_trainer,\n",
        "        'anomaly_detector': anomaly_detector,\n",
        "        'best_model_name': model_trainer.best_model_name,\n",
        "        'best_score': model_trainer.best_score\n",
        "    }\n",
        "\n",
        "def evaluate_website(url):\n",
        "    \"\"\"Fonction qui appelle l'analyseur de site web\"\"\"\n",
        "    try:\n",
        "        # Cr√©ation de l'objet analyseur\n",
        "        analyzer = WebsiteAnalyzer()\n",
        "\n",
        "        # Analyse du site web\n",
        "        result = analyzer.analyze_website(url)\n",
        "\n",
        "        return result\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            'error': f'Erreur lors de l\\'√©valuation: {str(e)}',\n",
        "            'url': url,\n",
        "            'scan_date': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "def handle_missing_values(self, df):\n",
        "    \"\"\"G√®re les valeurs manquantes dans le DataFrame\"\"\"\n",
        "    if self.config['handle_missing'] == 'drop':\n",
        "        df = df.dropna()\n",
        "    elif self.config['handle_missing'] == 'fill_empty':\n",
        "        # Fill missing values with empty strings for string columns only\n",
        "        string_cols = df.select_dtypes(include=['object']).columns\n",
        "        df[string_cols] = df[string_cols].fillna('')\n",
        "        # Impute numeric columns with 0\n",
        "        numeric_cols = df.select_dtypes(include=np.number).columns\n",
        "        df[numeric_cols] = df[numeric_cols].fillna(0)\n",
        "    elif self.config['handle_missing'] == 'fill_median':\n",
        "        for col in df.select_dtypes(include=np.number).columns:\n",
        "             df[col] = df[col].fillna(df[col].median())\n",
        "    return df"
      ],
      "metadata": {
        "id": "_gCxGGY--n-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SECTION 4: DEEP LEARNING POUR ANALYSE DE VULN√âRABILIT√âS\n"
      ],
      "metadata": {
        "id": "U9vedEmqYTcA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepLearningClassifier:\n",
        "    \"\"\"Classe pour l'utilisation de mod√®les de deep learning pour la classification de vuln√©rabilit√©s\"\"\"\n",
        "\n",
        "    def __init__(self, config=None):\n",
        "        self.config = config or {\n",
        "            'enabled': False,\n",
        "            'model_type': 'mlp',  # 'mlp', 'lstm', 'cnn'\n",
        "            'embedding_size': 100,\n",
        "            'lstm_units': 64,\n",
        "            'dense_units': [128, 64],\n",
        "            'dropout_rate': 0.3,\n",
        "            'learning_rate': 0.001,\n",
        "            'batch_size': 32,\n",
        "            'epochs': 50,\n",
        "            'early_stopping': True,\n",
        "            'patience': 5\n",
        "        }\n",
        "\n",
        "        self.model = None\n",
        "        self.history = None\n",
        "\n",
        "    def build_mlp_model(self, input_shape, num_classes):\n",
        "        \"\"\"Construit un mod√®le MLP pour la classification\"\"\"\n",
        "        model = Sequential()\n",
        "\n",
        "        # Couche d'entr√©e\n",
        "        model.add(Dense(self.config['dense_units'][0], activation='relu', input_shape=(input_shape,)))\n",
        "        model.add(Dropout(self.config['dropout_rate']))\n",
        "\n",
        "        # Couches cach√©es\n",
        "        for units in self.config['dense_units'][1:]:\n",
        "            model.add(Dense(units, activation='relu'))\n",
        "            model.add(Dropout(self.config['dropout_rate']))\n",
        "\n",
        "        # Couche de sortie\n",
        "        model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "        # Compilation\n",
        "        model.compile(\n",
        "            optimizer=tf.keras.optimizers.Adam(learning_rate=self.config['learning_rate']),\n",
        "            loss='sparse_categorical_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "        return model\n",
        "\n",
        "    def train(self, X, y):\n",
        "        \"\"\"Entra√Æne le mod√®le de deep learning\"\"\"\n",
        "        if not self.config['enabled']:\n",
        "            print(\"‚ö†Ô∏è Le mod√®le de deep learning n'est pas activ√© dans la configuration.\")\n",
        "            return None\n",
        "\n",
        "        print(\"\\nüß† Entra√Ænement du mod√®le de deep learning...\")\n",
        "\n",
        "        # Division des donn√©es\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "        # Construction du mod√®le\n",
        "        if self.config['model_type'] == 'mlp':\n",
        "            self.model = self.build_mlp_model(X.shape[1], len(np.unique(y)))\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è Type de mod√®le non pris en charge: {self.config['model_type']}\")\n",
        "            return None\n",
        "\n",
        "        # Callbacks\n",
        "        callbacks = []\n",
        "        if self.config['early_stopping']:\n",
        "            early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "                monitor='val_loss',\n",
        "                patience=self.config['patience'],\n",
        "                restore_best_weights=True\n",
        "            )\n",
        "            callbacks.append(early_stopping)\n",
        "\n",
        "        # Entra√Ænement\n",
        "        self.history = self.model.fit(\n",
        "            X_train, y_train,\n",
        "            batch_size=self.config['batch_size'],\n",
        "            epochs=self.config['epochs'],\n",
        "            validation_data=(X_test, y_test),\n",
        "            callbacks=callbacks,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        # √âvaluation\n",
        "        loss, accuracy = self.model.evaluate(X_test, y_test)\n",
        "        print(f\"   ‚úì Loss: {loss:.4f}\")\n",
        "        print(f\"   ‚úì Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "        # Pr√©dictions\n",
        "        y_pred = np.argmax(self.model.predict(X_test), axis=1)\n",
        "\n",
        "        # Rapport de classification\n",
        "        print(\"\\n   üìä Rapport de classification:\")\n",
        "        print(classification_report(y_test, y_pred))\n",
        "\n",
        "        # Matrice de confusion\n",
        "        cm = confusion_matrix(y_test, y_pred)\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                   xticklabels=['Faible', 'Mod√©r√©', 'Important', 'Critique'],\n",
        "                   yticklabels=['Faible', 'Mod√©r√©', 'Important', 'Critique'])\n",
        "        plt.title('Matrice de confusion - Deep Learning')\n",
        "        plt.ylabel('Valeur r√©elle')\n",
        "        plt.xlabel('Valeur pr√©dite')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('models/confusion_matrix_deep_learning.png')\n",
        "        plt.close()\n",
        "\n",
        "        # Courbe d'apprentissage\n",
        "        plt.figure(figsize=(12, 5))\n",
        "\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(self.history.history['accuracy'])\n",
        "        plt.plot(self.history.history['val_accuracy'])\n",
        "        plt.title('Pr√©cision du mod√®le')\n",
        "        plt.ylabel('Pr√©cision')\n",
        "        plt.xlabel('√âpoque')\n",
        "        plt.legend(['Train', 'Validation'], loc='lower right')\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(self.history.history['loss'])\n",
        "        plt.plot(self.history.history['val_loss'])\n",
        "        plt.title('Perte du mod√®le')\n",
        "        plt.ylabel('Perte')\n",
        "        plt.xlabel('√âpoque')\n",
        "        plt.legend(['Train', 'Validation'], loc='upper right')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('models/learning_curve_deep_learning.png')\n",
        "        plt.close()\n",
        "\n",
        "        # Sauvegarde du mod√®le\n",
        "        self.model.save('models/deep_learning_model')\n",
        "        print(\"   ‚úì Mod√®le de deep learning enregistr√©\")\n",
        "\n",
        "        return self.history"
      ],
      "metadata": {
        "id": "he8ZLH_r-wOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SECTION 5: ANALYSE DE SITES WEB EN TEMPS R√âEL\n"
      ],
      "metadata": {
        "id": "Ho0pdFbcYX57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import socket\n",
        "import ssl\n",
        "import re\n",
        "from datetime import datetime\n",
        "import json\n",
        "import warnings\n",
        "import random  # Pour simuler des r√©sultats al√©atoires\n",
        "\n",
        "# Ignorer les avertissements li√©s √† l'absence de v√©rification SSL pour les requ√™tes\n",
        "warnings.filterwarnings('ignore', message='Unverified HTTPS request')\n",
        "\n",
        "class WebsiteAnalyzer:\n",
        "    def __init__(self):\n",
        "        # Dans cette version de d√©monstration, nous utilisons des mod√®les simul√©s\n",
        "        self.risk_model = self.DummyModel()\n",
        "        self.anomaly_model = self.DummyAnomalyModel()\n",
        "        self.scaler = self.DummyScaler()\n",
        "\n",
        "    # Classes internes pour simuler les mod√®les de ML\n",
        "    class DummyModel:\n",
        "        def predict(self, X):\n",
        "            # Simuler un niveau de risque (0=Faible, 1=Mod√©r√©, 2=Important, 3=Critique)\n",
        "            return np.array([random.randint(0, 3)])\n",
        "\n",
        "        def predict_proba(self, X):\n",
        "            # Simuler des probabilit√©s pour chaque niveau\n",
        "            probs = np.random.random(4)\n",
        "            return np.array([probs / probs.sum()])\n",
        "\n",
        "    class DummyScaler:\n",
        "        def transform(self, X):\n",
        "            # Ne fait rien, retourne simplement les donn√©es\n",
        "            return X\n",
        "\n",
        "    class DummyAnomalyModel:\n",
        "        def decision_function(self, X):\n",
        "            # Simuler un score d'anomalie entre -0.5 et 0.5\n",
        "            return np.array([random.uniform(-0.5, 0.5)])\n",
        "\n",
        "        def predict(self, X):\n",
        "            # Simuler une d√©tection d'anomalie (1=normal, -1=anomalie)\n",
        "            return np.array([random.choice([1, -1])])\n",
        "\n",
        "    def extract_website_features(self, url):\n",
        "        \"\"\"Extrait les caract√©ristiques d'un site web pour l'analyse\"\"\"\n",
        "        # Normalisation de l'URL\n",
        "        if not url.startswith(('http://', 'https://')):\n",
        "            url = 'https://' + url\n",
        "\n",
        "        domain = url.split('/')[2] if '//' in url else url.split('/')[0]\n",
        "        print(f\"\\nüåê Analyse du site web: {url}\")\n",
        "\n",
        "        # Caract√©ristiques par d√©faut (cas o√π l'analyse √©choue)\n",
        "        features = {\n",
        "            'has_windows': 0,\n",
        "            'has_linux': 0,\n",
        "            'has_macos': 0,\n",
        "            'has_android': 0,\n",
        "            'has_ios': 0,\n",
        "            'has_wordpress': 0,\n",
        "            'has_joomla': 0,\n",
        "            'has_drupal': 0,\n",
        "            'has_oracle': 0,\n",
        "            'has_microsoft': 0,\n",
        "            'has_adobe': 0,\n",
        "            'has_plugin': 0,\n",
        "            'has_server': 1,  # Par d√©faut, un site web a un serveur\n",
        "            'has_web': 1,     # Par d√©faut, c'est un site web\n",
        "            'has_mobile': 0,\n",
        "            'has_iot': 0,\n",
        "            'has_cloud': 0,\n",
        "            'has_injection': 0,\n",
        "            'has_xss': 0,\n",
        "            'has_sql': 0,\n",
        "            'has_authentication': 0,\n",
        "            'has_authorization': 0,\n",
        "            'has_privilege': 0,\n",
        "            'has_code_execution': 0,\n",
        "            'has_arbitrary_code': 0,\n",
        "            'has_buffer': 0,\n",
        "            'has_overflow': 0,\n",
        "            'has_dos': 0,\n",
        "            'has_csrf': 0,\n",
        "            'has_path_traversal': 0,\n",
        "            'has_file_inclusion': 0,\n",
        "            'has_encryption': 0,\n",
        "            'has_ssl': 0,\n",
        "            'has_mitm': 0,\n",
        "            'has_bypass': 0,\n",
        "            'has_backdoor': 0,\n",
        "            'has_zero_day': 0,\n",
        "            'risk_level': 0,\n",
        "            'impact_level': 0\n",
        "        }\n",
        "\n",
        "        detected_vulnerabilities = []\n",
        "        security_recommendations = []\n",
        "\n",
        "        try:\n",
        "            # Pr√©paration de l'en-t√™te des requ√™tes\n",
        "            headers = {\n",
        "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "            }\n",
        "\n",
        "            # V√©rification SSL\n",
        "            print(\"   - V√©rification du certificat SSL...\")\n",
        "            try:\n",
        "                context = ssl.create_default_context()\n",
        "                with socket.create_connection((domain, 443), timeout=10) as sock:\n",
        "                    with context.wrap_socket(sock, server_hostname=domain) as ssock:\n",
        "                        cert = ssock.getpeercert()\n",
        "\n",
        "                        # Le certificat existe\n",
        "                        features['has_ssl'] = 1\n",
        "\n",
        "                        # V√©rification de la date d'expiration\n",
        "                        expiry_date = datetime.strptime(cert['notAfter'], '%b %d %H:%M:%S %Y %Z')\n",
        "                        ssl_expiry = expiry_date < datetime.now()\n",
        "                        if ssl_expiry:\n",
        "                            detected_vulnerabilities.append({\n",
        "                                'name': 'Certificat SSL expir√©',\n",
        "                                'description': f\"Le certificat SSL a expir√© le {expiry_date.strftime('%Y-%m-%d')}\",\n",
        "                                'severity': 'Important'\n",
        "                            })\n",
        "                            security_recommendations.append({\n",
        "                                'title': 'Renouveler le certificat SSL',\n",
        "                                'description': 'Contactez votre fournisseur de certificat ou utilisez Let\\'s Encrypt pour renouveler votre certificat SSL.',\n",
        "                                'priority': 'Haute'\n",
        "                            })\n",
        "                        print(f\"      ‚úì Certificat SSL valide: {'Non' if ssl_expiry else 'Oui'}\")\n",
        "            except Exception as e:\n",
        "                features['has_ssl'] = 0\n",
        "                detected_vulnerabilities.append({\n",
        "                    'name': 'Absence de SSL/TLS',\n",
        "                    'description': 'Le site n\\'utilise pas de connexion s√©curis√©e (HTTPS) ou le certificat n\\'est pas accessible',\n",
        "                    'severity': 'Critique'\n",
        "                })\n",
        "                security_recommendations.append({\n",
        "                    'title': 'Mettre en place HTTPS',\n",
        "                    'description': 'Installez un certificat SSL via Let\\'s Encrypt ou un autre fournisseur.',\n",
        "                    'priority': 'Critique'\n",
        "                })\n",
        "                print(f\"      ‚ö†Ô∏è Pas de certificat SSL ou erreur: {e}\")\n",
        "\n",
        "            # R√©cup√©ration de la page\n",
        "            print(\"   - R√©cup√©ration du contenu de la page...\")\n",
        "            response = requests.get(url, headers=headers, timeout=10, verify=False)\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "            # Simulation de d√©tection de vuln√©rabilit√©s\n",
        "            # Pour la d√©monstration, nous ajoutons quelques vuln√©rabilit√©s al√©atoires\n",
        "            vuln_types = [\n",
        "                ('has_xss', 'Vuln√©rabilit√© XSS potentielle', 'Scripts externes suspects d√©tect√©s'),\n",
        "                ('has_authentication', 'Formulaire d\\'identification non s√©curis√©', 'Le formulaire d\\'authentification pourrait √™tre vuln√©rable'),\n",
        "                ('has_sql', 'Injection SQL potentielle', 'Param√®tres URL non sanitis√©s d√©tect√©s'),\n",
        "                ('has_csrf', 'Protection CSRF manquante', 'Manque de jetons CSRF sur les formulaires'),\n",
        "                ('has_path_traversal', 'Travers√©e de chemin possible', 'Protection insuffisante contre les attaques de travers√©e de chemin')\n",
        "            ]\n",
        "\n",
        "            # Simuler al√©atoirement 1 √† 3 vuln√©rabilit√©s\n",
        "            num_vulnerabilities = random.randint(1, 3)\n",
        "            selected_vulnerabilities = random.sample(vuln_types, num_vulnerabilities)\n",
        "\n",
        "            severities = ['Faible', 'Mod√©r√©', 'Important', 'Critique']\n",
        "            priorities = ['Basse', 'Moyenne', 'Haute', 'Critique']\n",
        "\n",
        "            for vuln_type, name, desc in selected_vulnerabilities:\n",
        "                features[vuln_type] = 1\n",
        "                severity = random.choice(severities)\n",
        "                priority = random.choice(priorities)\n",
        "\n",
        "                detected_vulnerabilities.append({\n",
        "                    'name': name,\n",
        "                    'description': desc,\n",
        "                    'severity': severity\n",
        "                })\n",
        "\n",
        "                security_recommendations.append({\n",
        "                    'title': f'Corriger {name.lower()}',\n",
        "                    'description': f'Mettre en place des mesures pour prot√©ger contre {name.lower()}',\n",
        "                    'priority': priority\n",
        "                })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Erreur lors de l'analyse du site: {e}\")\n",
        "            # En cas d'erreur, ajouter une vuln√©rabilit√© g√©n√©rique\n",
        "            detected_vulnerabilities.append({\n",
        "                'name': 'Erreur lors de l\\'analyse',\n",
        "                'description': f'Impossible d\\'analyser correctement le site: {str(e)}',\n",
        "                'severity': 'Inconnu'\n",
        "            })\n",
        "            security_recommendations.append({\n",
        "                'title': 'V√©rifier l\\'accessibilit√© du site',\n",
        "                'description': 'Assurez-vous que le site est accessible et r√©pond correctement aux requ√™tes HTTP.',\n",
        "                'priority': 'Haute'\n",
        "            })\n",
        "\n",
        "        return features, detected_vulnerabilities, security_recommendations\n",
        "\n",
        "    def analyze_website(self, url):\n",
        "        \"\"\"Analyse compl√®te d'un site web et pr√©diction des risques\"\"\"\n",
        "        try:\n",
        "            # Extraction des caract√©ristiques\n",
        "            features, detected_vulnerabilities, security_recommendations = self.extract_website_features(url)\n",
        "\n",
        "            # Conversion des caract√©ristiques en format attendu par le mod√®le\n",
        "            X = np.array([[features[key] for key in features]])\n",
        "\n",
        "            print(f\"Nombre de caract√©ristiques extraites : {X.shape[1]}\")  # D√©bogage\n",
        "\n",
        "            # Normalisation\n",
        "            X_scaled = self.scaler.transform(X)\n",
        "\n",
        "            # Pr√©diction du niveau de risque\n",
        "            print(\"\\nüîÆ Pr√©diction des risques de s√©curit√©...\")\n",
        "            risk_level = self.risk_model.predict(X_scaled)[0]\n",
        "\n",
        "            # Probabilit√©s pour chaque classe\n",
        "            risk_proba = self.risk_model.predict_proba(X_scaled)[0]\n",
        "\n",
        "            # D√©tection d'anomalies\n",
        "            anomaly_score = self.anomaly_model.decision_function(X_scaled)[0]\n",
        "            is_anomaly = self.anomaly_model.predict(X_scaled)[0] == -1\n",
        "\n",
        "            # Mappage du niveau de risque num√©rique au texte\n",
        "            risk_level_map = {\n",
        "                0: 'Faible',\n",
        "                1: 'Mod√©r√©',\n",
        "                2: 'Important',\n",
        "                3: 'Critique'\n",
        "            }\n",
        "\n",
        "            # Calcul du score global (0-100, o√π 100 = risque √©lev√©)\n",
        "            risk_score_pct = int(((risk_level / 3) * 60) +\n",
        "                             (len(detected_vulnerabilities) * 3) +\n",
        "                             (20 if is_anomaly else 0))\n",
        "\n",
        "            # Plafonnement √† 100\n",
        "            risk_score_pct = min(100, risk_score_pct)\n",
        "\n",
        "            # R√©sultat\n",
        "            result = {\n",
        "                'url': url,\n",
        "                'scan_date': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "                'risk_level_numeric': int(risk_level),\n",
        "                'risk_level_text': risk_level_map.get(int(risk_level), 'Inconnu'),\n",
        "                'risk_score': risk_score_pct,\n",
        "                'is_anomaly': bool(is_anomaly),\n",
        "                'anomaly_score': float(anomaly_score),\n",
        "                'vulnerability_count': len(detected_vulnerabilities),\n",
        "                'vulnerabilities': detected_vulnerabilities,\n",
        "                'recommendations': security_recommendations,\n",
        "                'probability': {\n",
        "                    'Faible': float(risk_proba[0]) if len(risk_proba) > 0 else 0,\n",
        "                    'Mod√©r√©': float(risk_proba[1]) if len(risk_proba) > 1 else 0,\n",
        "                    'Important': float(risk_proba[2]) if len(risk_proba) > 2 else 0,\n",
        "                    'Critique': float(risk_proba[3]) if len(risk_proba) > 3 else 0\n",
        "                }\n",
        "            }\n",
        "\n",
        "            # Affichage du r√©sultat\n",
        "            print(f\"\\nüìä R√©sultats de l'analyse:\")\n",
        "            print(f\"   ‚úì URL: {result['url']}\")\n",
        "            print(f\"   ‚úì Niveau de risque: {result['risk_level_text']} ({result['risk_score']}%)\")\n",
        "            print(f\"   ‚úì Anomalie d√©tect√©e: {'Oui' if result['is_anomaly'] else 'Non'}\")\n",
        "            print(f\"   ‚úì Nombre de vuln√©rabilit√©s: {result['vulnerability_count']}\")\n",
        "            print(\"   ‚úì Probabilit√©s par niveau de risque:\")\n",
        "            for level, prob in result['probability'].items():\n",
        "                print(f\"      - {level}: {prob:.2f}\")\n",
        "\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                'error': f'Erreur lors de l\\'analyse: {str(e)}',\n",
        "                'url': url,\n",
        "                'scan_date': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "            }\n",
        "\n",
        "def generate_security_report(result, output_format='json'):\n",
        "    \"\"\"\n",
        "    G√©n√®re un rapport de s√©curit√© au format JSON ou HTML\n",
        "\n",
        "    Args:\n",
        "        result: Dictionnaire contenant les r√©sultats de l'analyse\n",
        "        output_format: Format du rapport ('json' ou 'html')\n",
        "\n",
        "    Returns:\n",
        "        str: Nom du fichier de rapport g√©n√©r√©\n",
        "    \"\"\"\n",
        "    # V√©rification que le r√©sultat est valide\n",
        "    if not result or 'error' in result:\n",
        "        error_message = result.get('error', 'Erreur inconnue') if result else 'R√©sultat vide'\n",
        "        print(f\"\\n‚ö†Ô∏è Impossible de g√©n√©rer le rapport complet: {error_message}\")\n",
        "\n",
        "        # Cr√©er un rapport d'erreur minimal\n",
        "        result = {\n",
        "            'url': result.get('url', 'URL inconnue') if result else 'URL inconnue',\n",
        "            'scan_date': result.get('scan_date', datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")) if result else datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "            'error': error_message,\n",
        "            'risk_level_text': 'Inconnu',\n",
        "            'risk_score': 0,\n",
        "            'vulnerability_count': 0,\n",
        "            'vulnerabilities': [],\n",
        "            'recommendations': []\n",
        "        }\n",
        "\n",
        "    # S'assurer que toutes les cl√©s n√©cessaires existent\n",
        "    required_keys = ['url', 'scan_date', 'risk_level_text', 'risk_score', 'vulnerability_count',\n",
        "                   'vulnerabilities', 'recommendations']\n",
        "\n",
        "    for key in required_keys:\n",
        "        if key not in result:\n",
        "            result[key] = 'Non disponible' if key in ['url', 'scan_date', 'risk_level_text'] else 0 if key in ['risk_score', 'vulnerability_count'] else []\n",
        "\n",
        "    # Cr√©ation du r√©pertoire reports s'il n'existe pas\n",
        "    os.makedirs('reports', exist_ok=True)\n",
        "\n",
        "    # Cr√©ation du nom de fichier en fonction de l'URL\n",
        "    domain = result['url'].replace('https://', '').replace('http://', '').split('/')[0]\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "    print(f\"\\nüìù G√©n√©ration du rapport de s√©curit√©...\")\n",
        "\n",
        "    if output_format.lower() == 'json':\n",
        "        # G√©n√©ration du rapport JSON\n",
        "        filename = f\"reports/rapport_securite_{domain}_{timestamp}.json\"\n",
        "        with open(filename, 'w', encoding='utf-8') as f:\n",
        "            json.dump(result, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "    elif output_format.lower() == 'html':\n",
        "        # G√©n√©ration du rapport HTML avec un style moderne\n",
        "        filename = f\"reports/rapport_securite_{domain}_{timestamp}.html\"\n",
        "\n",
        "        # Pr√©paration des sections HTML pour les vuln√©rabilit√©s\n",
        "        vulns_html = \"\"\n",
        "        if result['vulnerabilities']:\n",
        "            for vuln in result['vulnerabilities']:\n",
        "                severity = vuln.get('severity', 'Mod√©r√©')\n",
        "                description = vuln.get('description', 'Aucune description disponible')\n",
        "                vulns_html += f\"\"\"\n",
        "                <div class=\"vuln-item\">\n",
        "                    <h4>{vuln['name']} <span class=\"severity severity-{severity}\">{severity}</span></h4>\n",
        "                    <p>{description}</p>\n",
        "                </div>\n",
        "                \"\"\"\n",
        "        else:\n",
        "            vulns_html = \"<p>Aucune vuln√©rabilit√© d√©tect√©e.</p>\"\n",
        "\n",
        "        # Pr√©paration des sections HTML pour les recommandations\n",
        "        recs_html = \"\"\n",
        "        if result['recommendations']:\n",
        "            for rec in result['recommendations']:\n",
        "                priority = rec.get('priority', 'Moyenne')\n",
        "                description = rec.get('description', 'Aucune description disponible')\n",
        "                recs_html += f\"\"\"\n",
        "                <div class=\"rec-item\">\n",
        "                    <h4>{rec['title']}</h4>\n",
        "                    <p><span class=\"priority priority-{priority}\">Priorit√©: {priority}</span></p>\n",
        "                    <p>{description}</p>\n",
        "                </div>\n",
        "                \"\"\"\n",
        "        else:\n",
        "            recs_html = \"<p>Aucune recommandation disponible.</p>\"\n",
        "\n",
        "        # D√©terminer la classe CSS pour le niveau de risque\n",
        "        risk_class = 'risk-low'\n",
        "        if result['risk_level_text'] == 'Critique':\n",
        "            risk_class = 'risk-critical'\n",
        "        elif result['risk_level_text'] == 'Important':\n",
        "            risk_class = 'risk-important'\n",
        "        elif result['risk_level_text'] == 'Mod√©r√©':\n",
        "            risk_class = 'risk-moderate'\n",
        "\n",
        "        html_content = f\"\"\"<!DOCTYPE html>\n",
        "<html lang=\"fr\">\n",
        "<head>\n",
        "    <meta charset=\"UTF-8\">\n",
        "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
        "    <title>Rapport de s√©curit√© - {result['url']}</title>\n",
        "    <style>\n",
        "        :root {{\n",
        "            --color-bg: #f5f8fa;\n",
        "            --color-text: #333;\n",
        "            --color-primary: #2563eb;\n",
        "            --color-secondary: #4b5563;\n",
        "            --color-accent: #60a5fa;\n",
        "            --color-success: #10b981;\n",
        "            --color-warning: #f59e0b;\n",
        "            --color-danger: #ef4444;\n",
        "            --color-info: #3b82f6;\n",
        "            --color-light: #f3f4f6;\n",
        "            --color-dark: #1f2937;\n",
        "            --color-white: #ffffff;\n",
        "            --shadow-sm: 0 1px 2px 0 rgba(0, 0, 0, 0.05);\n",
        "            --shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1), 0 2px 4px -1px rgba(0, 0, 0, 0.06);\n",
        "            --shadow-lg: 0 10px 15px -3px rgba(0, 0, 0, 0.1), 0 4px 6px -2px rgba(0, 0, 0, 0.05);\n",
        "            --rounded: 0.375rem;\n",
        "        }}\n",
        "\n",
        "        body {{\n",
        "            font-family: 'Segoe UI', system-ui, -apple-system, sans-serif;\n",
        "            line-height: 1.6;\n",
        "            color: var(--color-text);\n",
        "            background-color: var(--color-bg);\n",
        "            margin: 0;\n",
        "            padding: 0;\n",
        "        }}\n",
        "\n",
        "        .container {{\n",
        "            max-width: 1200px;\n",
        "            margin: 2rem auto;\n",
        "            padding: 0 1rem;\n",
        "        }}\n",
        "\n",
        "        header {{\n",
        "            background-color: var(--color-primary);\n",
        "            color: var(--color-white);\n",
        "            padding: 2rem 0;\n",
        "            margin-bottom: 2rem;\n",
        "            box-shadow: var(--shadow);\n",
        "        }}\n",
        "\n",
        "        header .container {{\n",
        "            display: flex;\n",
        "            justify-content: space-between;\n",
        "            align-items: center;\n",
        "            margin-top: 0;\n",
        "            margin-bottom: 0;\n",
        "        }}\n",
        "\n",
        "        h1, h2, h3, h4 {{\n",
        "            color: var(--color-dark);\n",
        "            margin-top: 1.5rem;\n",
        "            margin-bottom: 1rem;\n",
        "        }}\n",
        "\n",
        "        header h1 {{\n",
        "            color: var(--color-white);\n",
        "            margin: 0;\n",
        "            font-size: 1.8rem;\n",
        "        }}\n",
        "\n",
        "        .card {{\n",
        "            background-color: var(--color-white);\n",
        "            border-radius: var(--rounded);\n",
        "            padding: 1.5rem;\n",
        "            margin-bottom: 1.5rem;\n",
        "            box-shadow: var(--shadow);\n",
        "        }}\n",
        "\n",
        "        .summary {{\n",
        "            display: grid;\n",
        "            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));\n",
        "            gap: 1rem;\n",
        "            margin-bottom: 2rem;\n",
        "        }}\n",
        "\n",
        "        .summary-item {{\n",
        "            padding: 1rem;\n",
        "            border-radius: var(--rounded);\n",
        "            background-color: var(--color-white);\n",
        "            box-shadow: var(--shadow-sm);\n",
        "            text-align: center;\n",
        "        }}\n",
        "\n",
        "        .summary-item h3 {{\n",
        "            margin-top: 0;\n",
        "            color: var(--color-secondary);\n",
        "            font-size: 1rem;\n",
        "        }}\n",
        "\n",
        "        .summary-item p {{\n",
        "            font-size: 1.5rem;\n",
        "            font-weight: bold;\n",
        "            margin: 0.5rem 0;\n",
        "        }}\n",
        "\n",
        "        .vuln-item, .rec-item {{\n",
        "            border-left: 4px solid var(--color-secondary);\n",
        "            margin-bottom: 1rem;\n",
        "            padding: 1rem;\n",
        "            background-color: var(--color-light);\n",
        "            border-radius: 0 var(--rounded) var(--rounded) 0;\n",
        "        }}\n",
        "\n",
        "        .vuln-item h4, .rec-item h4 {{\n",
        "            margin-top: 0;\n",
        "            margin-bottom: 0.5rem;\n",
        "        }}\n",
        "\n",
        "        .severity {{\n",
        "            display: inline-block;\n",
        "            padding: 0.25rem 0.75rem;\n",
        "            border-radius: 2rem;\n",
        "            font-size: 0.75rem;\n",
        "            font-weight: bold;\n",
        "            margin-left: 0.5rem;\n",
        "            color: var(--color-white);\n",
        "        }}\n",
        "\n",
        "        .severity-Critique {{\n",
        "            background-color: var(--color-danger);\n",
        "        }}\n",
        "\n",
        "        .severity-Important {{\n",
        "            background-color: var(--color-warning);\n",
        "        }}\n",
        "\n",
        "        .severity-Mod√©r√© {{\n",
        "            background-color: var(--color-info);\n",
        "        }}\n",
        "\n",
        "        .severity-Faible {{\n",
        "            background-color: var(--color-success);\n",
        "        }}\n",
        "\n",
        "        .risk-score {{\n",
        "            font-size: 1.2rem;\n",
        "            font-weight: bold;\n",
        "            color: var(--color-white);\n",
        "            padding: 0.5rem 1rem;\n",
        "            border-radius: var(--rounded);\n",
        "            display: inline-block;\n",
        "        }}\n",
        "\n",
        "        .risk-low {{\n",
        "            background-color: var(--color-success);\n",
        "        }}\n",
        "\n",
        "        .risk-moderate {{\n",
        "            background-color: var(--color-info);\n",
        "        }}\n",
        "\n",
        "        .risk-important {{\n",
        "            background-color: var(--color-warning);\n",
        "        }}\n",
        "\n",
        "        .risk-critical {{\n",
        "            background-color: var(--color-danger);\n",
        "        }}\n",
        "\n",
        "        .priority {{\n",
        "            font-weight: bold;\n",
        "        }}\n",
        "\n",
        "        .priority-Critique, .priority-Haute {{\n",
        "            color: var(--color-danger);\n",
        "        }}\n",
        "\n",
        "        .priority-Moyenne {{\n",
        "            color: var(--color-warning);\n",
        "        }}\n",
        "\n",
        "        .priority-Basse {{\n",
        "            color: var(--color-info);\n",
        "        }}\n",
        "\n",
        "        footer {{\n",
        "            text-align: center;\n",
        "            margin-top: 2rem;\n",
        "            padding: 1rem 0;\n",
        "            color: var(--color-secondary);\n",
        "            font-size: 0.875rem;\n",
        "        }}\n",
        "\n",
        "        @media (max-width: 768px) {{\n",
        "            .summary {{\n",
        "                grid-template-columns: 1fr;\n",
        "            }}\n",
        "        }}\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "    <header>\n",
        "        <div class=\"container\">\n",
        "            <h1>Rapport de s√©curit√© web</h1>\n",
        "            <p>{result['scan_date']}</p>\n",
        "        </div>\n",
        "    </header>\n",
        "\n",
        "    <div class=\"container\">\n",
        "        <div class=\"card\">\n",
        "            <h2>Informations g√©n√©rales</h2>\n",
        "            <p><strong>URL analys√©e:</strong> {result['url']}</p>\n",
        "            <p><strong>Date d'analyse:</strong> {result['scan_date']}</p>\n",
        "            <p>\n",
        "                <strong>Niveau de risque:</strong>\n",
        "                <span class=\"risk-score {risk_class}\">\n",
        "                    {result['risk_level_text']} ({result['risk_score']}%)\n",
        "                </span>\n",
        "            </p>\n",
        "        </div>\n",
        "\n",
        "        <div class=\"summary\">\n",
        "            <div class=\"summary-item\">\n",
        "                <h3>Niveau de risque</h3>\n",
        "                <p>{result['risk_level_text']}</p>\n",
        "            </div>\n",
        "            <div class=\"summary-item\">\n",
        "                <h3>Score de risque</h3>\n",
        "                <p>{result['risk_score']}%</p>\n",
        "            </div>\n",
        "            <div class=\"summary-item\">\n",
        "                <h3>Vuln√©rabilit√©s d√©tect√©es</h3>\n",
        "                <p>{result['vulnerability_count']}</p>\n",
        "            </div>\n",
        "        </div>\n",
        "\n",
        "        <div class=\"card\">\n",
        "            <h2>Vuln√©rabilit√©s d√©tect√©es</h2>\n",
        "            {vulns_html}\n",
        "        </div>\n",
        "\n",
        "        <div class=\"card\">\n",
        "            <h2>Recommandations de s√©curit√©</h2>\n",
        "            {recs_html}\n",
        "        </div>\n",
        "    </div>\n",
        "\n",
        "    <footer>\n",
        "        <div class=\"container\">\n",
        "            <p>Rapport g√©n√©r√© automatiquement le {result['scan_date']}</p>\n",
        "            <p>¬© {datetime.now().year} Syst√®me d'analyse de vuln√©rabilit√©s web</p>\n",
        "        </div>\n",
        "    </footer>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "        with open(filename, 'w', encoding='utf-8') as f:\n",
        "            f.write(html_content)\n",
        "\n",
        "    else:\n",
        "        print(f\"\\n‚ö†Ô∏è Format de rapport non support√©: {output_format}\")\n",
        "        return None\n",
        "\n",
        "    print(f\"\\n‚úÖ Rapport g√©n√©r√© avec succ√®s: {filename}\")\n",
        "    return filename\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def evaluate_website(url):\n",
        "    \"\"\"Fonction qui appelle l'analyseur de site web\"\"\"\n",
        "    try:\n",
        "        # Cr√©ation de l'objet analyseur\n",
        "        analyzer = WebsiteAnalyzer()\n",
        "\n",
        "        # Analyse du site web\n",
        "        result = analyzer.analyze_website(url)\n",
        "\n",
        "        return result\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            'error': f'Erreur lors de l\\'√©valuation: {str(e)}',\n",
        "            'url': url,\n",
        "            'scan_date': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        }\n",
        "\n",
        "def main():\n",
        "    \"\"\"Fonction principale du programme\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"üöÄ D√âMARRAGE DU SYST√àME DE PR√âDICTION DE VULN√âRABILIT√âS WEB\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Mode d'ex√©cution (entra√Ænement ou analyse)\n",
        "    mode = input(\"\\nMode d'ex√©cution :\\n1. Entra√Ænement des mod√®les\\n2. Analyse d'un site web\\nChoix (1/2): \").strip()\n",
        "\n",
        "    if mode == '1':\n",
        "        # Mode entra√Ænement (simul√© pour cet exemple)\n",
        "        print(\"\\nüîÑ Mode: Entra√Ænement des mod√®les\")\n",
        "        print(\"\\n‚ùó La fonctionnalit√© d'entra√Ænement est d√©sactiv√©e dans cette version de d√©monstration.\")\n",
        "\n",
        "    elif mode == '2':\n",
        "        # Mode analyse\n",
        "        print(\"\\nüîç Mode: Analyse d'un site web\")\n",
        "\n",
        "        # Demande de l'URL √† analyser\n",
        "        url = input(\"\\nEntrez l'URL du site √† analyser: \").strip()\n",
        "\n",
        "        # Analyse du site\n",
        "        result = evaluate_website(url)\n",
        "\n",
        "        # G√©n√©ration du rapport\n",
        "        report_format = input(\"\\nFormat du rapport (json/html): \").strip().lower()\n",
        "        if report_format not in ['json', 'html']:\n",
        "            report_format = 'json'\n",
        "\n",
        "        report_file = generate_security_report(result, report_format)\n",
        "\n",
        "    else:\n",
        "        print(\"\\n‚ö†Ô∏è Mode non reconnu. Fin du programme.\")\n",
        "\n",
        "    print(\"\\n‚úÖ Programme termin√© avec succ√®s!\")\n",
        "\n",
        "# Si ce fichier est ex√©cut√© directement\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9xNahtWQZaC",
        "outputId": "40bb6ca7-01cb-4cda-a9c9-287977fb2e32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "üöÄ D√âMARRAGE DU SYST√àME DE PR√âDICTION DE VULN√âRABILIT√âS WEB\n",
            "================================================================================\n",
            "\n",
            "Mode d'ex√©cution :\n",
            "1. Entra√Ænement des mod√®les\n",
            "2. Analyse d'un site web\n",
            "Choix (1/2): 2\n",
            "\n",
            "üîç Mode: Analyse d'un site web\n",
            "\n",
            "Entrez l'URL du site √† analyser: https://www.fbi.gov/services\n",
            "\n",
            "üåê Analyse du site web: https://www.fbi.gov/services\n",
            "   - V√©rification du certificat SSL...\n",
            "      ‚úì Certificat SSL valide: Oui\n",
            "   - R√©cup√©ration du contenu de la page...\n",
            "Nombre de caract√©ristiques extraites : 39\n",
            "\n",
            "üîÆ Pr√©diction des risques de s√©curit√©...\n",
            "\n",
            "üìä R√©sultats de l'analyse:\n",
            "   ‚úì URL: https://www.fbi.gov/services\n",
            "   ‚úì Niveau de risque: Important (46%)\n",
            "   ‚úì Anomalie d√©tect√©e: Non\n",
            "   ‚úì Nombre de vuln√©rabilit√©s: 2\n",
            "   ‚úì Probabilit√©s par niveau de risque:\n",
            "      - Faible: 0.08\n",
            "      - Mod√©r√©: 0.39\n",
            "      - Important: 0.26\n",
            "      - Critique: 0.26\n",
            "\n",
            "Format du rapport (json/html): html\n",
            "\n",
            "üìù G√©n√©ration du rapport de s√©curit√©...\n",
            "\n",
            "‚úÖ Rapport g√©n√©r√© avec succ√®s: reports/rapport_securite_www.fbi.gov_20250506_220937.html\n",
            "\n",
            "‚úÖ Programme termin√© avec succ√®s!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4zhlSG29Cin-",
        "outputId": "7efc2c8b-a722-42c9-c4f3-22db03660626"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 179, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/req_command.py\", line 67, in wrapper\n",
            "    return func(self, options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/commands/install.py\", line 447, in run\n",
            "    conflicts = self._determine_conflicts(to_install)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/commands/install.py\", line 578, in _determine_conflicts\n",
            "    return check_install_conflicts(to_install)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/operations/check.py\", line 101, in check_install_conflicts\n",
            "    package_set, _ = create_package_set_from_installed()\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/operations/check.py\", line 43, in create_package_set_from_installed\n",
            "    package_set[name] = PackageDetails(dist.version, dependencies)\n",
            "                                       ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/metadata/importlib/_dists.py\", line 175, in version\n",
            "    return parse_version(self._dist.version)\n",
            "                         ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/importlib/metadata/__init__.py\", line 632, in version\n",
            "    return self.metadata['Version']\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/importlib/metadata/__init__.py\", line 617, in metadata\n",
            "    return _adapters.Message(email.message_from_string(text))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/importlib/metadata/_adapters.py\", line 36, in __init__\n",
            "    self._headers = self._repair_headers()\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/importlib/metadata/_adapters.py\", line 49, in _repair_headers\n",
            "    headers = [(key, redent(value)) for key, value in vars(self)['_headers']]\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/importlib/metadata/_adapters.py\", line 49, in <listcomp>\n",
            "    headers = [(key, redent(value)) for key, value in vars(self)['_headers']]\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 10, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/main.py\", line 80, in main\n",
            "    return command.main(cmd_args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 100, in main\n",
            "    return self._main(args)\n",
            "           ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 232, in _main\n",
            "    return run(options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 215, in exc_logging_wrapper\n",
            "    logger.critical(\"Operation cancelled by user\")\n",
            "  File \"/usr/lib/python3.11/logging/__init__.py\", line 1536, in critical\n",
            "    self._log(CRITICAL, msg, args, **kwargs)\n",
            "  File \"/usr/lib/python3.11/logging/__init__.py\", line 1610, in _log\n",
            "    def _log(self, level, msg, args, exc_info=None, extra=None, stack_info=False,\n",
            "\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os # Add this line at the beginning of your script to import the os module\n",
        "\n",
        "# Fonction principale\n",
        "if __name__ == \"__main__\":\n",
        "    # Chemin vers le fichier de donn√©es\n",
        "    data_file = \"/content/rapports_dgssi_securite_cleaned.csv\"\n",
        "\n",
        "    # Configuration personnalis√©e (optionnelle)\n",
        "    custom_config = {\n",
        "        'preprocessing': {\n",
        "            'feature_selection': True,\n",
        "            'n_features': 25,\n",
        "            'apply_smote': True\n",
        "        },\n",
        "        'models': {\n",
        "            'xgboost': {\n",
        "                'grid_search': True\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Exemple d'utilisation - Entra√Ænement complet\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"üöÄ D√âMARRAGE DU SYST√àME DE PR√âDICTION DE VULN√âRABILIT√âS WEB\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Mode d'ex√©cution (entra√Ænement ou analyse)\n",
        "    mode = input(\"\\nMode d'ex√©cution :\\n1. Entra√Ænement des mod√®les\\n2. Analyse d'un site web\\nChoix (1/2): \").strip()\n",
        "\n",
        "    if mode == '1':\n",
        "        # Mode entra√Ænement\n",
        "        print(\"\\nüîÑ Mode: Entra√Ænement des mod√®les\")\n",
        "\n",
        "        # V√©rification de l'existence du fichier de donn√©es\n",
        "        if not os.path.exists(data_file):\n",
        "            data_file = input(f\"\\n‚ö†Ô∏è Fichier {data_file} non trouv√©. Veuillez entrer le chemin du fichier de donn√©es: \").strip()\n",
        "\n",
        "        # Entra√Ænement du pipeline complet\n",
        "        pipeline_results = train_complete_pipeline(data_file, custom_config)\n",
        "\n",
        "        print(f\"\\nüèÜ Meilleur mod√®le: {pipeline_results['best_model_name']} (Score: {pipeline_results['best_score']:.4f})\")\n",
        "\n",
        "    elif mode == '2':\n",
        "        # Mode analyse\n",
        "        print(\"\\nüîç Mode: Analyse d'un site web\")\n",
        "\n",
        "        # Demande de l'URL √† analyser\n",
        "        url = input(\"\\nEntrez l'URL du site √† analyser: \").strip()\n",
        "\n",
        "        # Analyse du site\n",
        "        result = evaluate_website(url)\n",
        "\n",
        "        # G√©n√©ration du rapport\n",
        "        report_format = input(\"\\nFormat du rapport (json/html): \").strip().lower()\n",
        "        if report_format not in ['json', 'html']:\n",
        "            report_format = 'json'\n",
        "\n",
        "        report_file = generate_security_report(result, report_format)\n",
        "\n",
        "    else:\n",
        "        print(\"\\n‚ö†Ô∏è Mode non reconnu. Fin du programme.\")\n",
        "\n",
        "    print(\"\\n‚úÖ Programme termin√© avec succ√®s!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8iWaNCnBtfC",
        "outputId": "9db42443-f4e1-4924-8906-9946cfe5c8b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "üöÄ D√âMARRAGE DU SYST√àME DE PR√âDICTION DE VULN√âRABILIT√âS WEB\n",
            "================================================================================\n",
            "\n",
            "Mode d'ex√©cution :\n",
            "1. Entra√Ænement des mod√®les\n",
            "2. Analyse d'un site web\n",
            "Choix (1/2): 1\n",
            "\n",
            "üîÑ Mode: Entra√Ænement des mod√®les\n",
            "\n",
            "üìÇ Cr√©ation de la structure de r√©pertoires...\n",
            "   ‚úì Structure de r√©pertoires cr√©√©e\n",
            "\n",
            "üìÇ Chargement des donn√©es depuis: /content/rapports_dgssi_securite_cleaned.csv...\n",
            "   ‚úì Donn√©es charg√©es: 1517 entr√©es, 9 colonnes\n",
            "   - Pr√©traitement des donn√©es...\n",
            "   ‚úì Donn√©es pr√©trait√©es.\n",
            "\n",
            "üîß Extraction des caract√©ristiques...\n",
            "   ‚ö†Ô∏è Valeurs manquantes d√©tect√©es dans Risk Level Numeric. Remplacement par 0...\n",
            "   - Utilisation de la colonne 'Combined_Text' pour l'extraction textuelle\n",
            "   ‚úì Caract√©ristiques textuelles extraites: 1000 dimensions\n",
            "   - Extraction des caract√©ristiques bas√©es sur les r√®gles...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "   üîÑ Traitement des entr√©es: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1517/1517 [00:00<00:00, 7354.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ‚úì Caract√©ristiques bas√©es sur les r√®gles extraites: 39 dimensions\n",
            "   ‚ö†Ô∏è Valeurs manquantes d√©tect√©es dans les caract√©ristiques. Remplacement par 0...\n",
            "   ‚úì Scaling appliqu√©: standard\n",
            "   - S√©lection des 25 meilleures caract√©ristiques...\n",
            "   ‚úì S√©lection de caract√©ristiques appliqu√©e: 25 dimensions retenues\n",
            "   üìã Caract√©ristiques s√©lectionn√©es: ['has_windows', 'has_linux', 'has_macos', 'has_ios', 'has_wordpress', 'has_joomla', 'has_drupal', 'has_adobe', 'has_plugin', 'has_server', 'has_web', 'has_cloud', 'has_xss', 'has_sql', 'has_authentication', 'has_authorization', 'has_privilege', 'has_code_execution', 'has_arbitrary_code', 'has_overflow', 'has_dos', 'has_path_traversal', 'has_bypass', 'has_zero_day', 'impact_level']\n",
            "\n",
            "‚öñÔ∏è Application de techniques d'√©quilibrage des classes...\n",
            "   - Distribution initiale des classes: {0.0: 14, 1.0: 218, 2.0: 817, 3.0: 468}\n",
            "   ‚úì Distribution apr√®s SMOTE: {0.0: 817, 1.0: 817, 2.0: 817, 3.0: 817}\n",
            "\n",
            "ü§ñ Entra√Ænement et √©valuation des mod√®les...\n",
            "   - Donn√©es divis√©es: 2614 exemples d'entra√Ænement, 654 exemples de test\n",
            "\n",
            "   üîÑ Entra√Ænement du mod√®le: random_forest...\n",
            "      ‚úì Entra√Ænement termin√© en 0.45 secondes\n",
            "      ‚úì Accuracy: 0.9434\n",
            "      ‚úì F1-Score: 0.9435\n",
            "\n",
            "      üìä Rapport de classification:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.99      1.00      1.00       164\n",
            "         1.0       0.99      0.93      0.96       163\n",
            "         2.0       0.92      0.87      0.89       164\n",
            "         3.0       0.88      0.97      0.92       163\n",
            "\n",
            "    accuracy                           0.94       654\n",
            "   macro avg       0.95      0.94      0.94       654\n",
            "weighted avg       0.95      0.94      0.94       654\n",
            "\n",
            "\n",
            "   üîÑ Entra√Ænement du mod√®le: xgboost...\n",
            "      ‚úì Entra√Ænement termin√© en 0.34 secondes\n",
            "      ‚úì Accuracy: 0.9465\n",
            "      ‚úì F1-Score: 0.9465\n",
            "\n",
            "      üìä Rapport de classification:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.99      1.00      1.00       164\n",
            "         1.0       0.99      0.94      0.97       163\n",
            "         2.0       0.93      0.87      0.90       164\n",
            "         3.0       0.88      0.98      0.92       163\n",
            "\n",
            "    accuracy                           0.95       654\n",
            "   macro avg       0.95      0.95      0.95       654\n",
            "weighted avg       0.95      0.95      0.95       654\n",
            "\n",
            "\n",
            "   üîÑ Entra√Ænement du mod√®le: gradient_boosting...\n",
            "      ‚úì Entra√Ænement termin√© en 1.19 secondes\n",
            "      ‚úì Accuracy: 0.9450\n",
            "      ‚úì F1-Score: 0.9449\n",
            "\n",
            "      üìä Rapport de classification:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.99      1.00      1.00       164\n",
            "         1.0       0.99      0.94      0.97       163\n",
            "         2.0       0.92      0.87      0.89       164\n",
            "         3.0       0.88      0.98      0.92       163\n",
            "\n",
            "    accuracy                           0.94       654\n",
            "   macro avg       0.95      0.94      0.94       654\n",
            "weighted avg       0.95      0.94      0.94       654\n",
            "\n",
            "\n",
            "   üîÑ Entra√Ænement du mod√®le: logistic_regression...\n",
            "      ‚úì Entra√Ænement termin√© en 0.04 secondes\n",
            "      ‚úì Accuracy: 0.9297\n",
            "      ‚úì F1-Score: 0.9293\n",
            "\n",
            "      üìä Rapport de classification:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.99      1.00      1.00       164\n",
            "         1.0       0.97      0.90      0.94       163\n",
            "         2.0       0.90      0.84      0.86       164\n",
            "         3.0       0.86      0.98      0.92       163\n",
            "\n",
            "    accuracy                           0.93       654\n",
            "   macro avg       0.93      0.93      0.93       654\n",
            "weighted avg       0.93      0.93      0.93       654\n",
            "\n",
            "\n",
            "   üîÑ Entra√Ænement du mod√®le: voting_ensemble...\n",
            "      ‚úì Entra√Ænement termin√© en 1.67 secondes\n",
            "      ‚úì Accuracy: 0.9450\n",
            "      ‚úì F1-Score: 0.9448\n",
            "\n",
            "      üìä Rapport de classification:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.99      1.00      1.00       164\n",
            "         1.0       0.99      0.94      0.97       163\n",
            "         2.0       0.93      0.86      0.89       164\n",
            "         3.0       0.88      0.98      0.92       163\n",
            "\n",
            "    accuracy                           0.94       654\n",
            "   macro avg       0.95      0.95      0.94       654\n",
            "weighted avg       0.95      0.94      0.94       654\n",
            "\n",
            "\n",
            "üìã R√©sum√© des performances des mod√®les:\n",
            "                Mod√®le  Accuracy  F1-Score  Temps (s)\n",
            "1              xgboost  0.946483  0.946458   0.335834\n",
            "2    gradient_boosting  0.944954  0.944940   1.187076\n",
            "4      voting_ensemble  0.944954  0.944825   1.665514\n",
            "0        random_forest  0.943425  0.943510   0.454913\n",
            "3  logistic_regression  0.929664  0.929294   0.037286\n",
            "\n",
            "üèÜ Meilleur mod√®le: xgboost (F1-Score: 0.9465)\n",
            "\n",
            "üîç Entra√Ænement des mod√®les de d√©tection d'anomalies...\n",
            "   - Entra√Ænement d'Isolation Forest...\n",
            "   ‚úì Mod√®le Isolation Forest enregistr√©\n",
            "   - Entra√Ænement de Local Outlier Factor...\n",
            "   ‚úì Mod√®le Local Outlier Factor enregistr√©\n",
            "\n",
            "üìä Visualisation des anomalies d√©tect√©es par isolation_forest...\n",
            "   ‚úì 164 anomalies d√©tect√©es (5.02%)\n",
            "\n",
            "‚úÖ Pipeline d'entra√Ænement complet termin√©!\n",
            "\n",
            "üèÜ Meilleur mod√®le: xgboost (Score: 0.9465)\n",
            "\n",
            "‚úÖ Programme termin√© avec succ√®s!\n"
          ]
        }
      ]
    }
  ]
}